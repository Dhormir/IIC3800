{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3800 Tópicos en CC - NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- numpy 1.20.3\n",
    "- nltk 3.7\n",
    "- gensim 4.1.2\n",
    "- gcsfs 2023.3.0\n",
    "- protobuf 3.20.3\n",
    "- keras 2.9.0\n",
    "- tensorflow 2.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critica</th>\n",
       "      <th>nota</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bueno, bajo mi gusto, otro fracaso más de DC. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Es tan terrible que podría funcionar como paro...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tengo una tradición desde hace más de 5 años. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No entiendo como nadie tiene la cara de presen...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La primera entrega de Wonder Woman (2017) no m...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>\"Shrek\" es sin lugar a dudas una de las mejore...</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/6/4945...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>Muy buena e incluso diría, inteligente comedia...</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/3/9420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>Cuando una película consigue hacer que algo ta...</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/3/9420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>Una gran comedia estupida que cumple su funció...</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/3/9420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>Primer \"peplum\" moderno que sorprendió a la ma...</td>\n",
       "      <td>10</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/3/3920...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                critica  nota  \\\n",
       "0     Bueno, bajo mi gusto, otro fracaso más de DC. ...     3   \n",
       "1     Es tan terrible que podría funcionar como paro...     1   \n",
       "2     Tengo una tradición desde hace más de 5 años. ...     2   \n",
       "3     No entiendo como nadie tiene la cara de presen...     1   \n",
       "4     La primera entrega de Wonder Woman (2017) no m...     4   \n",
       "...                                                 ...   ...   \n",
       "4795  \"Shrek\" es sin lugar a dudas una de las mejore...     9   \n",
       "4796  Muy buena e incluso diría, inteligente comedia...     8   \n",
       "4797  Cuando una película consigue hacer que algo ta...     7   \n",
       "4798  Una gran comedia estupida que cumple su funció...     8   \n",
       "4799  Primer \"peplum\" moderno que sorprendió a la ma...    10   \n",
       "\n",
       "                                                    url  \n",
       "0     https://www.filmaffinity.com/es/reviews/1/4208...  \n",
       "1     https://www.filmaffinity.com/es/reviews/1/4208...  \n",
       "2     https://www.filmaffinity.com/es/reviews/1/4208...  \n",
       "3     https://www.filmaffinity.com/es/reviews/1/4208...  \n",
       "4     https://www.filmaffinity.com/es/reviews/1/4208...  \n",
       "...                                                 ...  \n",
       "4795  https://www.filmaffinity.com/es/reviews/6/4945...  \n",
       "4796  https://www.filmaffinity.com/es/reviews/3/9420...  \n",
       "4797  https://www.filmaffinity.com/es/reviews/3/9420...  \n",
       "4798  https://www.filmaffinity.com/es/reviews/3/9420...  \n",
       "4799  https://www.filmaffinity.com/es/reviews/3/3920...  \n",
       "\n",
       "[4800 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"film_affinity.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________\n",
    "\n",
    "## Actividad en clase\n",
    "\n",
    "Construya un clasificador de sentimiento en **castellano** usando el dataset FilmAffinity y Spacy. Para esto haga lo siguiente:\n",
    "\n",
    "- Cree la columna 'sentiment' a partir de **nota**. Sentiment debe considerar dos clases: **positivo** y **negativo**. Observe que nota va de 1 a 10. \n",
    "- Preprocese el texto del campo **critica** usando lo que hemos visto en clases. Note que deberá cambiar algunos procesos ya que el texto está en **castellano**.\n",
    "- Particione el dataset en particiones de training/testing (90/10).\n",
    "- Ajuste un modelo **word2vec** al dataset usando gensim (vea la clase de ayer).\n",
    "- Cree un clasificador AWE usando estos vectores (vea la clase de hoy). \n",
    "- Evalúe el clasificador sobre la particion de test.\n",
    "- Cuanto termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que las L otorgan un bono en la nota final de la asignatura.\n",
    "\n",
    "\n",
    "***Tiene hasta el final de la clase.***\n",
    "\n",
    "_________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\") # a spanish-based nlp model\n",
    "REGX_USERNAME = r\"@[A-Za-z0-9$-_@.&+]+\"\n",
    "\n",
    "def preprocessing(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(REGX_USERNAME, ' ', text)\n",
    "  tokens = [token.text for token in nlp(text)]\n",
    "  tokens = [t for t in tokens if t not in STOP_WORDS and t not in string.punctuation and len(t) > 2]\n",
    "  tokens = [t for t in tokens if not t.isdigit()]\n",
    "\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "\n",
    "df[\"text_clean\"] = df[\"critica\"].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = df.apply(lambda x: 0 if int(x[\"nota\"]) < 5  else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critica</th>\n",
       "      <th>nota</th>\n",
       "      <th>url</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bueno, bajo mi gusto, otro fracaso más de DC. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "      <td>gusto fracaso empezó año aves presa acaba año ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Es tan terrible que podría funcionar como paro...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "      <td>terrible funcionar parodia sobreactuada record...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tengo una tradición desde hace más de 5 años. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "      <td>tradición años diciembre cine blockbuster peli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No entiendo como nadie tiene la cara de presen...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "      <td>entiendo cara presentar película entiendo crít...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La primera entrega de Wonder Woman (2017) no m...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.filmaffinity.com/es/reviews/1/4208...</td>\n",
       "      <td>entrega wonder woman pareció maravilla contrar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             critica  nota  \\\n",
       "0  Bueno, bajo mi gusto, otro fracaso más de DC. ...     3   \n",
       "1  Es tan terrible que podría funcionar como paro...     1   \n",
       "2  Tengo una tradición desde hace más de 5 años. ...     2   \n",
       "3  No entiendo como nadie tiene la cara de presen...     1   \n",
       "4  La primera entrega de Wonder Woman (2017) no m...     4   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.filmaffinity.com/es/reviews/1/4208...   \n",
       "1  https://www.filmaffinity.com/es/reviews/1/4208...   \n",
       "2  https://www.filmaffinity.com/es/reviews/1/4208...   \n",
       "3  https://www.filmaffinity.com/es/reviews/1/4208...   \n",
       "4  https://www.filmaffinity.com/es/reviews/1/4208...   \n",
       "\n",
       "                                          text_clean  sentiment  \n",
       "0  gusto fracaso empezó año aves presa acaba año ...          0  \n",
       "1  terrible funcionar parodia sobreactuada record...          0  \n",
       "2  tradición años diciembre cine blockbuster peli...          0  \n",
       "3  entiendo cara presentar película entiendo crít...          0  \n",
       "4  entrega wonder woman pareció maravilla contrar...          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(df[[\"text_clean\", \"sentiment\"]].sample(frac=1).itertuples(index=False, name=None))\n",
    "train_data = dataset[:4320]  # 90%\n",
    "test_data = dataset[4320:] # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = list(list(zip(*train_data))[0])\n",
    "X_test_text = list(list(zip(*test_data))[0])\n",
    "y_train = list(list(zip(*train_data))[1])\n",
    "y_test = list(list(zip(*test_data))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "\n",
    "# Initialize tokenizer\n",
    "# It's also possible to try with a stemmer or to mix a stemmer and a lemmatizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [t.lower() for t in tokenizer.tokenize(sentence) if len(t) > 2]\n",
    "        words += tokens\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = []\n",
    "\n",
    "for raw_text in X_train_text:\n",
    "    words = tokenize(raw_text)\n",
    "    corpus_words.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=corpus_words, vector_size=100, window=5, min_count=1, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('william', 0.99941086769104),\n",
       " ('pierre', 0.9984567165374756),\n",
       " ('eligi', 0.9984337091445923),\n",
       " ('temple', 0.9976115822792053),\n",
       " ('periodista', 0.99753737449646),\n",
       " ('devil', 0.9974064826965332),\n",
       " ('debidamente', 0.9973728060722351),\n",
       " ('tibetano', 0.9973520636558533),\n",
       " ('village', 0.9973311424255371),\n",
       " ('selznick', 0.9972866177558899)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar_cosmul(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4320, 50), (480, 50))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_tokens = 50 ## Hyperparameter, input length\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_text+X_test_text)\n",
    "\n",
    "## Vectorizing data to keep 50 words per sample.\n",
    "X_train_vect = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)\n",
    "X_test_vect  = pad_sequences(tokenizer.texts_to_sequences(X_test_text), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)\n",
    "\n",
    "\n",
    "X_train_vect.shape, X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectors_w2v = np.asarray(model.wv.vectors)\n",
    "labels_w2v = np.asarray(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 100\n",
    "\n",
    "w2v_embeddings = np.zeros((len(tokenizer.index_word)+1, embed_len))\n",
    "\n",
    "for idx, word in tokenizer.index_word.items():\n",
    "    if word in labels_w2v:\n",
    "        w2v_embeddings[idx] = vectors_w2v[int(np.where(labels_w2v == word)[0][0])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 50, 100)           5690700   \n",
      "                                                                 \n",
      " tf.math.reduce_mean_3 (TFOp  (None, 100)              0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               12928     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,712,014\n",
      "Trainable params: 21,314\n",
      "Non-trainable params: 5,690,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "\n",
    "inputs = Input(shape=(max_tokens, ))\n",
    "embeddings_layer = Embedding(input_dim=len(tokenizer.index_word)+1, output_dim=embed_len,\n",
    "                             input_length=max_tokens, trainable=False, weights=[w2v_embeddings])\n",
    "dense1 = Dense(128, activation=\"relu\")\n",
    "dense2 = Dense(64, activation=\"relu\")\n",
    "dense3 = Dense(2, activation=\"softmax\")\n",
    "\n",
    "x = embeddings_layer(inputs)\n",
    "x = tensorflow.reduce_mean(x, axis=1) ### Averaged embeddings of tokens of each example\n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "outputs = dense3(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "135/135 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6384\n",
      "Epoch 2/8\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.5761 - accuracy: 0.7097\n",
      "Epoch 3/8\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.5695 - accuracy: 0.7100\n",
      "Epoch 4/8\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.5633 - accuracy: 0.7146\n",
      "Epoch 5/8\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.5599 - accuracy: 0.7178\n",
      "Epoch 6/8\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.5546 - accuracy: 0.7188\n",
      "Epoch 7/8\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.5467 - accuracy: 0.7308\n",
      "Epoch 8/8\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.5424 - accuracy: 0.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f34ea19a280>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vect, y_train, batch_size=32, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 2ms/step\n",
      "Test Accuracy : 0.7395833333333334\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         POS       0.73      0.77      0.75       240\n",
      "         NEG       0.75      0.71      0.73       240\n",
      "\n",
      "    accuracy                           0.74       480\n",
      "   macro avg       0.74      0.74      0.74       480\n",
      "weighted avg       0.74      0.74      0.74       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_test = np.asarray(y_test)\n",
    "Y_preds = model.predict(X_test_vect).argmax(axis=-1)\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(y_test, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(y_test, Y_preds, target_names=['POS','NEG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
