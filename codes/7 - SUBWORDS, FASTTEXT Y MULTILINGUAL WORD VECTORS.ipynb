{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerÃ­as, python 3.8.10\n",
    "\n",
    "- numpy 1.20.3\n",
    "- nltk 3.7\n",
    "- gensim 4.1.2\n",
    "- keras 2.9.0\n",
    "- tensorflow 2.9.1\n",
    "- instant-distance 0.3.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_300 = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9390855431556702),\n",
       " ('queen-mother', 0.9078598618507385),\n",
       " ('king-', 0.8828966617584229),\n",
       " ('queen-consort', 0.882541835308075),\n",
       " ('child-king', 0.8680858016014099),\n",
       " ('monarch', 0.8670082688331604),\n",
       " ('ex-queen', 0.8654637932777405),\n",
       " ('princess', 0.8628991842269897),\n",
       " ('queen-', 0.8613532781600952),\n",
       " ('boy-king', 0.860465943813324)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_300.most_similar_cosmul(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, results = ft_300.evaluate_word_analogies('questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8827876424099353"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectors_ft = np.asarray(ft_300.vectors)\n",
    "labels_ft = np.asarray(ft_300.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "X_train_text, Y_train = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
    "X_test_text, Y_test  = fetch_20newsgroups(subset=\"test\", remove=('headers', 'footers', 'quotes'), return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "classes = np.unique(Y_train)\n",
    "\n",
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "# It's also possible to try with a stemmer or to mix a stemmer and a lemmatizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words and len(t) > 2]\n",
    "        words += tokens\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = []\n",
    "test_docs = []\n",
    "\n",
    "for raw_text in X_train_text:\n",
    "    text = tokenize(raw_text)\n",
    "    train_docs.append(text)\n",
    "    \n",
    "for raw_text in X_test_text:\n",
    "    text = tokenize(raw_text)\n",
    "    test_docs.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 50), (7532, 50))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_tokens = 50 ## Hyperparameter, input length\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_docs+test_docs)\n",
    "\n",
    "## Vectorizing data to keep 50 words per sample.\n",
    "X_train_vect = pad_sequences(tokenizer.texts_to_sequences(train_docs), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)\n",
    "X_test_vect  = pad_sequences(tokenizer.texts_to_sequences(test_docs), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)\n",
    "\n",
    "\n",
    "X_train_vect.shape, X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95077"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 300\n",
    "\n",
    "ft_embeddings = np.zeros((len(tokenizer.index_word)+1, embed_len))\n",
    "\n",
    "for idx, word in tokenizer.index_word.items():\n",
    "    if word in labels_ft:\n",
    "        ft_embeddings[idx] = vectors_ft[int(np.where(labels_ft == word)[0][0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 300)           28523400  \n",
      "                                                                 \n",
      " tf.math.reduce_mean (TFOpLa  (None, 300)              0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               38528     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                1300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,571,484\n",
      "Trainable params: 48,084\n",
      "Non-trainable params: 28,523,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "\n",
    "inputs = Input(shape=(max_tokens, ))\n",
    "embeddings_layer = Embedding(input_dim=len(tokenizer.index_word)+1, output_dim=embed_len,\n",
    "                             input_length=max_tokens, trainable=False, weights=[ft_embeddings])\n",
    "dense1 = Dense(128, activation=\"relu\")\n",
    "dense2 = Dense(64, activation=\"relu\")\n",
    "dense3 = Dense(len(classes), activation=\"softmax\")\n",
    "\n",
    "x = embeddings_layer(inputs)\n",
    "x = tensorflow.reduce_mean(x, axis=1) ### Averaged embeddings of tokens of each example\n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "outputs = dense3(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 2.5607 - accuracy: 0.2124\n",
      "Epoch 2/8\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 1.8645 - accuracy: 0.3940\n",
      "Epoch 3/8\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 1.6331 - accuracy: 0.4557\n",
      "Epoch 4/8\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 1.5205 - accuracy: 0.4972\n",
      "Epoch 5/8\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 1.4550 - accuracy: 0.5210\n",
      "Epoch 6/8\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 1.3962 - accuracy: 0.5438\n",
      "Epoch 7/8\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 1.3539 - accuracy: 0.5597\n",
      "Epoch 8/8\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 1.3186 - accuracy: 0.5733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77bb52cca0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vect, Y_train, batch_size=32, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 2ms/step\n",
      "Test Accuracy : 0.5350504514073288\n",
      "\n",
      "Classification Report : \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.25      0.31      0.27       319\n",
      "           comp.graphics       0.52      0.60      0.55       389\n",
      " comp.os.ms-windows.misc       0.48      0.34      0.39       394\n",
      "comp.sys.ibm.pc.hardware       0.53      0.17      0.25       392\n",
      "   comp.sys.mac.hardware       0.37      0.48      0.42       385\n",
      "          comp.windows.x       0.57      0.58      0.58       395\n",
      "            misc.forsale       0.63      0.62      0.62       390\n",
      "               rec.autos       0.73      0.50      0.59       396\n",
      "         rec.motorcycles       0.46      0.64      0.53       398\n",
      "      rec.sport.baseball       0.41      0.80      0.55       397\n",
      "        rec.sport.hockey       0.82      0.71      0.76       399\n",
      "               sci.crypt       0.71      0.59      0.64       396\n",
      "         sci.electronics       0.46      0.49      0.48       393\n",
      "                 sci.med       0.69      0.74      0.71       396\n",
      "               sci.space       0.73      0.59      0.65       394\n",
      "  soc.religion.christian       0.48      0.82      0.61       398\n",
      "      talk.politics.guns       0.48      0.56      0.52       364\n",
      "   talk.politics.mideast       0.88      0.64      0.74       376\n",
      "      talk.politics.misc       0.39      0.23      0.29       310\n",
      "      talk.religion.misc       0.21      0.02      0.04       251\n",
      "\n",
      "                accuracy                           0.54      7532\n",
      "               macro avg       0.54      0.52      0.51      7532\n",
      "            weighted avg       0.55      0.54      0.52      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "Y_preds = model.predict(X_test_vect).argmax(axis=-1)\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, Y_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilingual aligned word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver tÃ©cnica de word alignment en: https://arxiv.org/pdf/1710.04087.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGS = ('en', 'fr')\n",
    "LANG_REPLACE = '$$lang'\n",
    "WORD_MAP_PATH = f\"./data/{'_'.join(LANGS)}.json\"\n",
    "BUILT_IDX_PATH = f\"./data/{'_'.join(LANGS)}.idx\"\n",
    "DL_TEMPLATE = f\"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.{LANG_REPLACE}.align.vec\"\n",
    "\n",
    "points = []\n",
    "values = []\n",
    "word_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver documentaciÃ³n en: https://fasttext.cc/docs/en/aligned-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, aiohttp\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "  for lang in LANGS:\n",
    "    # Construct a url for each language\n",
    "    url = DL_TEMPLATE.replace(LANG_REPLACE, lang)\n",
    "\n",
    "    # Ensure the directory and files exist\n",
    "    os.makedirs(os.path.dirname(BUILT_IDX_PATH), exist_ok=True)\n",
    "\n",
    "    lineno = 0\n",
    "    async with session.get(url) as resp:\n",
    "      while True:\n",
    "        lineno += 1\n",
    "        line = await resp.content.readline()\n",
    "        if not line:\n",
    "          # EOF\n",
    "          break\n",
    "\n",
    "        linestr = line.decode('utf-8')\n",
    "        tokens = linestr.split(' ')\n",
    "\n",
    "        # The first token is the word and the rest\n",
    "        # are the embedding\n",
    "        value = tokens[0]\n",
    "        embedding = [float(p) for p in tokens[1:]]\n",
    "\n",
    "        # We only go from english to the other two languages\n",
    "        if lang == 'en':\n",
    "          word_map[value] = embedding\n",
    "        else:\n",
    "          # Don't index words that exist in english\n",
    "          # to improve the quality of the results.\n",
    "          if value in word_map:\n",
    "              continue\n",
    "\n",
    "          # We track values here to build the instant-distance index\n",
    "          # Every value is prepended with 2 character language code.\n",
    "          # This allows us to determine language output later.\n",
    "          values.append(lang + value)\n",
    "          points.append(embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver documentaciÃ³n del motor de vecinos cercanos en: https://github.com/InstantDomain/instant-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index... (this will take a while)\n"
     ]
    }
   ],
   "source": [
    "import instant_distance, json\n",
    "\n",
    "# Build the instant-distance index and dump it out to a file with .idx suffix\n",
    "print('Building index... (this will take a while)')\n",
    "hnsw = instant_distance.HnswMap.build(points, values, instant_distance.Config())\n",
    "hnsw.dump(BUILT_IDX_PATH)\n",
    "\n",
    "# Store the mapping from string to embedding in a .json file\n",
    "with open(WORD_MAP_PATH, 'w') as f:\n",
    "    json.dump(word_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frbonjours\n",
      "frbonjourÂ \n",
      "fr#bonjour\n",
      "frÂ bonjour\n",
      "frremerci\n",
      "frbonjoursg\n",
      "frbonsoir,\n",
      "frrebonjour\n",
      "fr>bonjour\n",
      "frbonjour,\n"
     ]
    }
   ],
   "source": [
    "word = 'hello'\n",
    "\n",
    "# Get an embedding for the given word\n",
    "embedding = word_map.get(word)\n",
    "if not embedding:\n",
    "  print(f\"Word not recognized: {word}\")\n",
    "  exit(1)\n",
    "\n",
    "hnsw = instant_distance.HnswMap.load(BUILT_IDX_PATH)\n",
    "search = instant_distance.Search()\n",
    "hnsw.search(embedding, search)\n",
    "\n",
    "# Print the results\n",
    "for result in list(search)[:10]:\n",
    "  # We know that the first two characters of the value is the language code\n",
    "  # from when we built the index.\n",
    "  print(result.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
