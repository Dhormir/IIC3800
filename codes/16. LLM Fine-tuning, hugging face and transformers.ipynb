{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5926f1a8",
   "metadata": {},
   "source": [
    "# IIC-3800 Tópicos en CC - NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6ea20",
   "metadata": {},
   "source": [
    "! pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2856cd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddaec61bae0d479c8c4e5bb5c0d78746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d059212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a36ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"language_modeling_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef81a46",
   "metadata": {},
   "source": [
    "Fine-tuning a DistilGPT2 con Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520f4802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/marce/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48a825161b6414b83d7435cbd733660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3b7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a40371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L.A.M.B. has participated in the Spring / Summer 2006 , 2007 , and 2008 New York Fashion Weeks . Stefani described her first line , which debuted on September 16 , 2005 , as \" a little Sound of Music , some Orange County chola girl , some Rasta , and a bit of The Great Gatsby . \" The highlights of the show were purple cars bouncing using hydraulics while Stefani 's song \" Wind It Up \" made its debut as the models walked the runway . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In January 2011 , one year after the quake , Oxfam published a report on the status of the recovery . According to the report , relief and recovery were at a standstill due to government inaction and indecision on the part of the donor countries . The report stated , \" One year on , only five percent of the rubble has been cleared and only 15 percent of the required basic and temporary houses have been built . House building on a large scale cannot be started before the enormous amount of rubble is cleared . The government and donors must prioritize this most basic step toward helping people return home \" . Robert Fox , executive director of Oxfam Canada , said \" The dysfunction has been aided unabated by the way the international community has organized itself , where pledges have been made and they haven 't followed through [ and ] where they come to the table with their own agendas and own priorities . Most donors provided funds for transitional housing but very little money for clearing rubble or repairing houses \" . Fox said that in many instances rubble removal \" means it was [ moved ] off someone 's property onto the road in front of the property \" . According to a UNICEF report , \" Still today more than one million people remain displaced , living in crowded camps where livelihoods , shelter and services are still hardly sufficient for children to stay healthy \" . Amnesty International reported that armed men were preying with impunity on girls and women in displacement camps , worsening the trauma of having lost homes , livelihoods and loved ones . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Their opponents in the second round were Hungarian champions Honvéd . A goal from Santillana ensured Real won the first leg 1 – 0 in Spain . Two goals from Laurie Cunningham and Francisco García Hernández secured a 2 – 0 victory in the second leg at Honvéd 's home ground the Bozsik Stadion , thus , winning the tie 3 – 0 on aggregate . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>= = Relationships with humans = = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6c101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e499c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return self.tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "325b7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21644b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_wrapper = TokenizerWrapper(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcc2be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-30009711dc059d99_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-6bbf9169baf7c5fb_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-207565d47927079f_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenizer_wrapper.tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05289e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "add2f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    block_size = 128\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae08f2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-8da755f5ffa8a975_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-fb04cb0963cad339_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-b98f68ea47ed009c_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc5d10f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Oz'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "320b3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd7df180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d12802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba4f4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18666\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7002\n",
      "  Number of trainable parameters = 81912576\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7002' max='7002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7002/7002 07:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.759600</td>\n",
       "      <td>3.665788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.651900</td>\n",
       "      <td>3.647420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.601400</td>\n",
       "      <td>3.642290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-500\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-500\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-1000\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-1000\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-1500\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-1500\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-2000\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-2000\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1931\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-2500\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-2500\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-2500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-3000\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-3000\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-3500\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-3500\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-3500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-4000\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-4000\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-4500\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-4500\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1931\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-5000\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-5000\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-5500\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-5500\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-5500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-6000\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-6000\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-6500\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-6500\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-6500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilgpt2-finetuned-wikitext2\\checkpoint-7000\n",
      "Configuration saved in distilgpt2-finetuned-wikitext2\\checkpoint-7000\\config.json\n",
      "Model weights saved in distilgpt2-finetuned-wikitext2\\checkpoint-7000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1931\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7002, training_loss=3.6941190673024, metrics={'train_runtime': 465.2762, 'train_samples_per_second': 120.354, 'train_steps_per_second': 15.049, 'total_flos': 1829011929956352.0, 'train_loss': 3.6941190673024, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e6d5b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilgpt2-finetuned.h5\n",
      "Configuration saved in distilgpt2-finetuned.h5\\config.json\n",
      "Model weights saved in distilgpt2-finetuned.h5\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"distilgpt2-finetuned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28b83f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file distilgpt2-finetuned.h5\\config.json\n",
      "You are using a model of type gpt2 to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_layers\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file distilgpt2-finetuned.h5\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilgpt2-finetuned.h5 were not used when initializing DistilBertModel: ['transformer.h.2.ln_2.bias', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.2.ln_1.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.wte.weight', 'transformer.h.5.ln_1.weight', 'transformer.h.4.attn.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.1.attn.masked_bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.wpe.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.0.ln_2.bias', 'lm_head.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.2.ln_2.weight', 'transformer.h.4.ln_1.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.ln_f.bias', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.5.ln_1.bias', 'transformer.h.0.ln_1.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.0.ln_2.weight', 'transformer.h.3.ln_1.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.0.attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.1.ln_2.bias', 'transformer.ln_f.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.4.ln_2.weight', 'transformer.h.3.attn.masked_bias', 'transformer.h.5.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.4.ln_1.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.2.attn.bias', 'transformer.h.1.attn.bias', 'transformer.h.3.ln_1.bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.5.attn.bias', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.4.ln_2.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.2.ln_1.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.0.attn.masked_bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at distilgpt2-finetuned.h5 and are newly initialized: ['transformer.layer.5.attention.out_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.3.ffn.lin1.bias', 'embeddings.LayerNorm.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.2.attention.out_lin.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'embeddings.position_embeddings.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.0.attention.q_lin.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertConfig, DistilBertModel\n",
    "\n",
    "model = DistilBertModel.from_pretrained(\"distilgpt2-finetuned.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea54b5",
   "metadata": {},
   "source": [
    "Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25dabccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilroberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa0b4276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-30009711dc059d99_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-6bbf9169baf7c5fb_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-207565d47927079f_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenized_datasets = datasets.map(tokenizer_wrapper.tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22aae613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-8da755f5ffa8a975_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-fb04cb0963cad339_*_of_00004.arrow\n",
      "Loading cached processed dataset at C:\\Users\\marce\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-b98f68ea47ed009c_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "105e683a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e141782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64f41e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82c7e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "525612f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18666\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7002\n",
      "  Number of trainable parameters = 82170201\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7002' max='7002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7002/7002 07:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.915600</td>\n",
       "      <td>5.779372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.670700</td>\n",
       "      <td>5.579891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.551200</td>\n",
       "      <td>5.498873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-500\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-500\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-1000\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-1000\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-1500\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-1500\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-2000\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-2000\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1931\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-2500\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-2500\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-2500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-3000\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-3000\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-3500\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-3500\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-3500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-4000\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-4000\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-4500\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-4500\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1931\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-5000\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-5000\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-5500\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-5500\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-5500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-6000\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-6000\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-6500\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-6500\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-6500\\pytorch_model.bin\n",
      "Saving model checkpoint to distilroberta-base-finetuned-wikitext2\\checkpoint-7000\n",
      "Configuration saved in distilroberta-base-finetuned-wikitext2\\checkpoint-7000\\config.json\n",
      "Model weights saved in distilroberta-base-finetuned-wikitext2\\checkpoint-7000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1931\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7002, training_loss=5.8092338602327, metrics={'train_runtime': 429.523, 'train_samples_per_second': 130.373, 'train_steps_per_second': 16.302, 'total_flos': 1856638981384704.0, 'train_loss': 5.8092338602327, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7721676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilroberta-finetuned.h5\n",
      "Configuration saved in distilroberta-finetuned.h5\\config.json\n",
      "Model weights saved in distilroberta-finetuned.h5\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"distilroberta-finetuned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "602580ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file distilroberta-finetuned.h5\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file distilroberta-finetuned.h5\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-finetuned.h5 were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at distilroberta-finetuned.h5 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained(\"distilroberta-finetuned.h5\")\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bef9f095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0287,  0.1037,  0.0497,  ..., -0.1419, -0.0878, -0.1035],\n",
       "         [-0.1012,  0.1349,  0.0487,  ..., -0.1946, -0.0503,  0.0087],\n",
       "         [-0.0154,  0.2017, -0.0121,  ..., -0.5496,  0.0703,  0.0294],\n",
       "         ...,\n",
       "         [-0.0099,  0.0509, -0.0503,  ...,  0.2083,  0.2238, -0.1072],\n",
       "         [-0.0450,  0.1117, -0.0041,  ..., -0.1812, -0.0819, -0.1107],\n",
       "         [-0.0152,  0.0821,  0.0414,  ..., -0.0479, -0.0595, -0.0274]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.7856e-02, -2.6244e-02,  3.7591e-02, -4.3138e-02, -6.1570e-04,\n",
       "          4.6531e-01,  1.3777e-01,  1.7208e-01, -4.9658e-01, -1.8586e-02,\n",
       "          3.8688e-01,  1.5355e-01, -3.2527e-01,  4.8366e-02,  1.6435e-01,\n",
       "         -3.6283e-01,  3.4307e-01, -2.0699e-01, -8.2975e-02,  7.2161e-02,\n",
       "         -2.0497e-02,  3.7040e-01, -2.3906e-01,  1.6113e-01,  5.1800e-01,\n",
       "         -4.1507e-01, -4.0727e-01, -5.0621e-01,  4.3897e-01,  2.9301e-01,\n",
       "          4.4141e-01, -6.1026e-01,  2.4793e-01,  2.2311e-01, -2.0458e-01,\n",
       "         -1.7310e-01,  3.2646e-01, -2.0962e-01,  5.3373e-02,  1.0884e-01,\n",
       "         -1.5899e-01, -6.2762e-01, -3.8246e-01,  2.0184e-01, -3.3099e-01,\n",
       "          6.2288e-01, -3.5441e-01,  3.1491e-01, -8.1206e-02,  2.0751e-02,\n",
       "         -4.4125e-01, -7.5856e-02,  4.2562e-01, -6.7481e-02, -2.5839e-01,\n",
       "         -2.0295e-02,  1.1789e-01, -3.3744e-02, -8.4076e-03, -2.1235e-01,\n",
       "         -3.4797e-01, -6.9801e-02,  3.0579e-01,  1.0946e-01, -3.1720e-03,\n",
       "          5.4221e-01, -2.7539e-01,  6.5427e-02,  1.2250e-02, -3.8006e-01,\n",
       "          4.0675e-01,  1.0993e-01,  3.5486e-01, -1.8357e-02, -4.2021e-01,\n",
       "         -2.5391e-01,  3.2454e-01,  1.7654e-01,  1.3183e-01, -3.1629e-01,\n",
       "          5.9023e-04, -1.1213e-01, -1.2040e-02,  9.5130e-02, -4.3699e-01,\n",
       "          1.8762e-01, -1.1693e-01, -3.4245e-01,  3.7944e-01, -1.5734e-01,\n",
       "          6.5500e-01, -8.9966e-02, -1.1680e-01,  7.3859e-02, -2.4428e-01,\n",
       "         -2.8420e-03, -1.7747e-03, -8.0245e-03,  1.0745e-01,  2.6902e-01,\n",
       "         -3.9761e-03,  9.1451e-03,  4.2071e-01, -2.7114e-01,  3.2045e-01,\n",
       "          4.7698e-01, -1.0607e-01,  1.6997e-01,  3.0674e-01,  3.7301e-01,\n",
       "         -5.9438e-01, -3.6097e-01,  3.8396e-01, -3.5487e-01,  2.8273e-01,\n",
       "          8.5566e-02, -5.5524e-01,  3.7335e-01,  3.4689e-01,  2.8639e-01,\n",
       "          2.3542e-01,  4.5626e-01, -2.5125e-01,  2.3269e-01,  2.0202e-03,\n",
       "          7.1424e-01,  4.5622e-01,  2.9912e-01,  1.4129e-01, -1.8173e-01,\n",
       "          1.5772e-01,  1.5140e-01, -1.1510e-01,  5.3690e-01,  4.7532e-01,\n",
       "          2.9522e-01, -5.4601e-02, -1.2734e-01,  7.2770e-01, -6.4617e-01,\n",
       "          1.8268e-01,  2.9181e-01,  4.7161e-01,  7.2105e-02,  3.5382e-01,\n",
       "         -3.2606e-01, -4.5085e-02, -8.7272e-02,  3.8687e-01, -5.3133e-01,\n",
       "          3.8124e-01,  2.0039e-01, -1.7871e-01, -5.0030e-02, -3.2730e-01,\n",
       "          1.7888e-01, -3.0599e-01,  1.7859e-01, -1.9798e-01, -5.0936e-01,\n",
       "          4.7081e-01,  5.3543e-02,  1.6102e-02, -4.4712e-01,  2.1133e-01,\n",
       "          1.1251e-01, -1.2093e-01,  3.4959e-01, -4.7215e-01,  3.6375e-01,\n",
       "          1.4946e-01,  4.9780e-02, -2.8217e-01, -4.1957e-03,  4.6992e-01,\n",
       "          5.0830e-01,  2.5808e-01, -1.2596e-01,  3.6638e-01, -1.0692e-01,\n",
       "         -4.5709e-01,  2.0176e-01,  3.8907e-01,  4.8786e-01,  2.0741e-01,\n",
       "          6.3207e-02, -8.3924e-03, -3.1969e-01, -1.7317e-01,  5.3530e-01,\n",
       "         -1.8950e-01, -2.2495e-01, -1.3378e-01,  5.3392e-02,  4.8119e-01,\n",
       "          2.6602e-01,  2.7146e-01, -2.7646e-02,  4.7258e-01, -1.7909e-01,\n",
       "         -1.0695e-01,  4.5667e-01,  1.3640e-01, -5.7177e-01,  3.0722e-01,\n",
       "          2.1383e-01, -3.2835e-01,  1.8596e-01, -3.6170e-02, -2.8028e-01,\n",
       "          2.0943e-03, -4.3850e-01, -5.4528e-01,  1.8515e-02, -3.2590e-02,\n",
       "         -1.1834e-01, -2.6451e-01, -3.7708e-01, -2.1034e-01,  3.4690e-01,\n",
       "          5.2974e-01, -5.9330e-02,  3.3215e-01, -5.1030e-01,  1.7799e-02,\n",
       "         -6.1048e-02, -3.7957e-01,  3.7121e-01,  4.3450e-02, -1.3924e-01,\n",
       "          1.9995e-01,  7.7967e-03,  6.1186e-02,  4.1915e-01, -5.4383e-02,\n",
       "         -1.1149e-01,  1.4316e-01,  1.5815e-01,  1.0116e-01, -2.6038e-02,\n",
       "         -2.0853e-01, -2.6137e-01,  2.3619e-01, -8.6290e-02, -7.3114e-02,\n",
       "          2.3780e-01, -5.3291e-01, -3.6345e-01, -1.5885e-01, -1.6954e-02,\n",
       "          5.1187e-02, -1.2095e-01, -7.3862e-02, -3.4788e-01,  3.5893e-04,\n",
       "         -6.3163e-02, -2.2688e-01,  5.3917e-02, -7.5732e-02,  1.3228e-01,\n",
       "         -6.3654e-02, -1.0379e-01,  1.0296e-01, -1.7678e-02, -2.3486e-01,\n",
       "         -3.9318e-02, -2.1316e-01,  5.7393e-01, -5.2467e-01, -2.8129e-01,\n",
       "          4.2504e-02, -3.3516e-01,  2.7780e-01, -2.8796e-01,  1.6040e-01,\n",
       "         -5.9808e-01, -1.8059e-01,  1.4904e-01, -5.3251e-02,  3.6545e-01,\n",
       "         -3.6345e-01,  2.1795e-02, -2.2502e-01,  1.2321e-01,  3.5980e-01,\n",
       "          3.1749e-01, -4.0180e-01,  9.9436e-02,  3.1322e-01,  2.5546e-01,\n",
       "         -5.0138e-01, -2.6280e-01, -8.5899e-02, -7.3603e-02,  4.8153e-01,\n",
       "         -2.9906e-01, -1.1058e-01,  1.7951e-01,  4.2900e-02, -3.4380e-01,\n",
       "          3.9996e-01, -6.6350e-01,  3.9727e-01,  1.6600e-01, -1.9526e-01,\n",
       "          1.7814e-01,  1.7856e-01, -1.9210e-01, -8.0111e-02,  5.8434e-01,\n",
       "          4.5736e-01,  4.5962e-01,  1.0875e-01,  2.2894e-02,  2.3387e-01,\n",
       "         -2.2361e-02,  3.7536e-01, -2.9201e-01, -3.8885e-01, -7.4017e-01,\n",
       "          4.7159e-01, -4.1587e-01, -1.5681e-02,  6.4847e-01,  1.4951e-01,\n",
       "          1.7825e-01, -2.6869e-01,  3.9142e-01,  2.3822e-01,  6.2026e-02,\n",
       "         -4.2564e-02,  4.5552e-01, -5.9494e-02, -1.2618e-01, -1.2905e-01,\n",
       "          2.5115e-01,  1.6155e-02,  1.0130e-01, -2.9712e-01, -3.3612e-01,\n",
       "          3.2682e-01, -4.6122e-01,  1.4107e-01, -1.1706e-01,  5.8850e-01,\n",
       "         -2.2085e-02, -2.2094e-01, -4.3153e-01, -2.8141e-01, -2.6084e-01,\n",
       "         -8.6256e-02,  2.7932e-01, -4.4268e-01, -1.1536e-01, -2.0420e-01,\n",
       "          2.0797e-01, -1.6176e-02, -4.6216e-01,  6.5106e-01, -3.9373e-02,\n",
       "         -1.9472e-01,  6.7406e-02, -1.8756e-01, -1.9304e-01,  7.3344e-02,\n",
       "          1.4526e-01,  2.0991e-01,  5.7609e-01, -2.7970e-01,  4.8010e-01,\n",
       "         -9.2847e-02,  8.7295e-02, -9.1867e-02, -4.6853e-01,  3.9043e-01,\n",
       "          7.0071e-01, -1.2932e-01,  3.5040e-01, -1.0798e-01,  1.7653e-01,\n",
       "         -5.7929e-02,  1.0633e-01, -7.1710e-01, -5.3202e-01, -1.9369e-02,\n",
       "          3.1999e-01,  1.9021e-02, -2.0365e-01,  1.8706e-01,  3.1275e-02,\n",
       "          2.1509e-01, -2.3023e-01,  2.7376e-02, -1.4398e-02, -2.7151e-01,\n",
       "         -1.1632e-01, -1.9048e-01,  1.8822e-01,  6.8072e-01,  4.0291e-01,\n",
       "          2.0622e-01,  8.4530e-02, -5.8588e-02, -5.5827e-01, -4.8359e-02,\n",
       "         -1.2066e-02, -2.3522e-02,  1.8271e-01, -7.6845e-02, -6.0577e-01,\n",
       "         -6.5298e-02,  1.3229e-02,  2.2688e-01,  4.1379e-01,  2.2069e-01,\n",
       "          4.0319e-01, -6.4070e-01,  1.2376e-02,  1.5627e-01,  1.5388e-01,\n",
       "         -1.1217e-01, -5.6181e-01, -2.1432e-01,  2.0932e-01,  5.0859e-01,\n",
       "         -9.2616e-02, -3.4809e-01, -1.7425e-01,  5.1211e-01,  2.9336e-01,\n",
       "          6.0488e-01,  1.8574e-01, -1.7913e-01, -3.3552e-01, -5.0266e-01,\n",
       "         -1.9090e-01,  5.1208e-02,  3.0607e-01, -7.1822e-02,  4.8614e-03,\n",
       "          1.6107e-02,  4.8715e-01,  3.3169e-01,  1.2582e-01, -1.8186e-01,\n",
       "         -3.2423e-02,  4.4848e-01,  5.7975e-02,  2.1597e-01, -2.6745e-01,\n",
       "          1.0860e-01, -2.2699e-01, -1.9372e-01, -6.9118e-01, -4.5160e-01,\n",
       "         -4.5272e-02,  2.6210e-01, -9.8763e-02, -3.3918e-01, -3.7905e-02,\n",
       "          1.5468e-01,  1.5159e-01,  4.3984e-01,  8.1459e-02, -5.3158e-01,\n",
       "         -2.8785e-02, -3.8697e-01,  5.1770e-01,  8.0754e-02,  4.5954e-01,\n",
       "          4.5480e-01,  1.9851e-01,  1.3966e-01, -1.9235e-02, -1.0395e-02,\n",
       "         -1.4135e-01,  1.6458e-01,  4.2410e-01, -1.7926e-01, -4.7277e-02,\n",
       "         -6.3180e-02, -1.1234e-01, -1.5926e-01,  6.6069e-01, -1.5270e-01,\n",
       "         -1.2969e-01,  5.3636e-02,  4.1313e-01,  4.7191e-01, -4.5091e-01,\n",
       "          1.9885e-01, -1.5872e-01,  3.5697e-01,  1.3227e-01,  4.1606e-01,\n",
       "         -3.1889e-01, -2.7163e-02, -1.2596e-01, -3.6100e-01,  4.6326e-01,\n",
       "         -2.6688e-01, -2.6810e-01, -2.5969e-01, -1.1819e-01, -3.7089e-01,\n",
       "          5.6958e-01,  2.9910e-01,  1.1110e-01,  6.4693e-02,  6.1335e-01,\n",
       "         -3.5547e-04,  2.5358e-01,  2.8932e-01,  3.4180e-01,  5.9239e-01,\n",
       "         -1.5563e-01, -4.3210e-01, -1.5570e-01, -3.3746e-01,  6.4248e-03,\n",
       "         -2.8498e-01, -3.3968e-01, -5.1827e-01,  4.0123e-01, -4.1610e-01,\n",
       "         -2.1490e-01, -2.1932e-01,  1.0113e-01, -2.2452e-01,  2.4890e-01,\n",
       "          1.4427e-01,  5.0795e-01, -8.8381e-02, -1.7887e-02, -4.0282e-01,\n",
       "         -3.9589e-01, -2.1480e-01,  4.8395e-01, -3.9327e-01,  7.5667e-02,\n",
       "          3.1146e-01,  2.5137e-01,  6.1071e-02, -2.1707e-01, -3.8831e-01,\n",
       "         -1.0968e-01,  2.7975e-01,  4.7033e-01,  2.8491e-01,  2.6224e-01,\n",
       "         -2.6482e-01,  2.8556e-01,  3.6021e-02,  2.1457e-01, -2.3416e-02,\n",
       "          9.7998e-02, -3.7796e-01, -3.0578e-01,  6.0623e-02, -4.3660e-01,\n",
       "          3.2336e-01, -4.7651e-01, -1.6463e-01,  2.3621e-01, -2.7379e-01,\n",
       "         -3.7257e-01,  2.4555e-01,  1.1023e-01,  2.7346e-02,  2.1355e-02,\n",
       "         -5.2470e-02,  6.6588e-01,  3.3358e-01, -5.7162e-02, -3.9981e-01,\n",
       "         -4.9088e-01,  6.2439e-01, -1.0132e-01, -1.7603e-01, -6.2099e-01,\n",
       "          1.3874e-01, -6.7593e-02, -2.3398e-01,  1.5573e-01,  2.3655e-01,\n",
       "         -1.5628e-01,  4.1627e-01,  2.1454e-02, -2.2026e-01, -5.9617e-01,\n",
       "          6.4081e-02,  1.5212e-01, -6.7871e-01,  1.5031e-01,  2.3831e-01,\n",
       "         -1.9263e-01, -2.5390e-03, -7.7494e-02,  3.2188e-01,  1.1748e-01,\n",
       "          3.2677e-01,  4.8148e-01, -1.9971e-01,  1.5936e-01, -2.6406e-01,\n",
       "         -2.5801e-01, -2.6323e-01, -2.6360e-01,  1.9122e-01, -1.1146e-01,\n",
       "          5.7728e-01,  1.9873e-01,  3.5902e-01, -4.0569e-01, -1.0501e-01,\n",
       "          1.7641e-01, -3.1401e-02, -1.4181e-01,  2.1267e-02, -3.2265e-01,\n",
       "         -3.0962e-01, -2.9471e-01,  1.3215e-01, -4.9671e-02,  5.7794e-03,\n",
       "         -4.1560e-01, -6.0405e-01,  9.0214e-02, -4.2392e-01,  2.5677e-01,\n",
       "          2.6463e-01,  2.8832e-01, -2.1639e-01,  1.5467e-01,  1.5458e-01,\n",
       "          2.1425e-01,  2.0502e-01, -1.1242e-01,  4.0917e-02,  1.4827e-01,\n",
       "          1.7523e-01, -2.2882e-01, -6.6662e-02,  3.5216e-02,  4.5551e-01,\n",
       "          2.0555e-01,  2.1406e-01,  8.9740e-02, -3.9070e-01,  4.6679e-01,\n",
       "         -9.9058e-02,  3.4704e-01, -4.8161e-02,  3.9401e-01,  4.1888e-01,\n",
       "         -1.5961e-02, -1.1031e-01,  2.8971e-01,  2.1700e-01, -8.1828e-02,\n",
       "         -4.3691e-01, -6.2111e-02, -2.2850e-01, -1.5109e-01, -5.3558e-01,\n",
       "         -1.6685e-01,  3.8981e-02, -6.9728e-01,  4.7007e-01,  3.3263e-01,\n",
       "          2.6678e-01,  2.8664e-01, -1.7276e-01, -8.3265e-03,  8.9335e-02,\n",
       "         -3.1812e-01,  2.2303e-01, -2.2966e-01,  8.7949e-02,  4.1685e-01,\n",
       "          5.3311e-01,  1.1535e-01,  3.7004e-01, -5.7717e-01,  4.1000e-02,\n",
       "         -1.1288e-01, -2.8040e-01, -3.3725e-01,  5.6667e-02,  5.7864e-01,\n",
       "          2.1121e-02, -2.0046e-01, -2.6716e-01, -1.3296e-01, -2.6680e-01,\n",
       "         -2.3294e-02,  3.8307e-02, -2.0081e-01, -2.6128e-01,  2.8764e-01,\n",
       "          3.7849e-01,  3.9236e-01,  7.7014e-02, -8.7629e-02,  2.2215e-01,\n",
       "          6.2008e-01, -2.1730e-01, -6.4844e-01,  3.9067e-01, -3.0832e-01,\n",
       "         -2.7797e-01, -4.4047e-01,  2.8961e-01,  8.5640e-02,  4.8432e-02,\n",
       "         -4.1863e-02, -3.4815e-01, -4.3361e-01,  4.7402e-01,  1.8038e-01,\n",
       "          2.6396e-01, -2.3743e-01,  1.9155e-01,  9.5029e-03,  3.1407e-01,\n",
       "         -5.3188e-03,  4.7578e-01, -2.9185e-01,  6.9661e-02,  6.4917e-02,\n",
       "          1.7810e-02, -2.8077e-01,  5.9075e-01,  1.4921e-01,  1.7688e-01,\n",
       "          1.0555e-01,  2.7455e-01,  2.3792e-02,  1.6506e-01, -1.7736e-02,\n",
       "         -2.6276e-01, -1.7260e-01, -4.5660e-01, -1.9606e-01, -7.0698e-01,\n",
       "         -1.0645e-02, -3.7493e-01,  4.2432e-02,  1.5801e-01, -4.8755e-01,\n",
       "          3.8082e-01,  5.0011e-01,  5.6573e-01, -4.1433e-01, -4.3237e-01,\n",
       "          9.6380e-02,  2.0832e-01, -1.6179e-01,  7.1751e-01,  1.0700e-01,\n",
       "          1.5214e-01, -2.9530e-01, -6.3225e-02, -1.4391e-01,  3.5232e-01,\n",
       "         -5.0630e-01,  1.5811e-01, -3.4056e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85de71a",
   "metadata": {},
   "source": [
    "Using pretrained LLMs (sentence transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa56bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\7dbbc90392e2f80f3d3c277d6e90027e55de9125\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\7dbbc90392e2f80f3d3c277d6e90027e55de9125\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\7dbbc90392e2f80f3d3c277d6e90027e55de9125\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\7dbbc90392e2f80f3d3c277d6e90027e55de9125\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\7dbbc90392e2f80f3d3c277d6e90027e55de9125\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\marce/.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\7dbbc90392e2f80f3d3c277d6e90027e55de9125\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "503491b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[ 6.7657e-02,  6.3496e-02,  4.8713e-02,  7.9305e-02,  3.7448e-02,\n",
      "          2.6528e-03,  3.9375e-02, -7.0985e-03,  5.9361e-02,  3.1537e-02,\n",
      "          6.0098e-02, -5.2905e-02,  4.0607e-02, -2.5931e-02,  2.9843e-02,\n",
      "          1.1269e-03,  7.3515e-02, -5.0382e-02, -1.2239e-01,  2.3703e-02,\n",
      "          2.9727e-02,  4.2477e-02,  2.5634e-02,  1.9952e-03, -5.6919e-02,\n",
      "         -2.7160e-02, -3.2904e-02,  6.6025e-02,  1.1901e-01, -4.5879e-02,\n",
      "         -7.2621e-02, -3.2584e-02,  5.2341e-02,  4.5055e-02,  8.2531e-03,\n",
      "          3.6702e-02, -1.3942e-02,  6.5392e-02, -2.6427e-02,  2.0638e-04,\n",
      "         -1.3664e-02, -3.6281e-02, -1.9504e-02, -2.8974e-02,  3.9427e-02,\n",
      "         -8.8409e-02,  2.6243e-03,  1.3671e-02,  4.8306e-02, -3.1157e-02,\n",
      "         -1.1733e-01, -5.1169e-02, -8.8529e-02, -2.1896e-02,  1.4299e-02,\n",
      "          4.4417e-02, -1.3482e-02,  7.4339e-02,  2.6638e-02, -1.9876e-02,\n",
      "          1.7919e-02, -1.0605e-02, -9.0426e-02,  2.1327e-02,  1.4120e-01,\n",
      "         -6.4717e-03, -1.4038e-03, -1.5361e-02, -8.7357e-02,  7.2217e-02,\n",
      "          2.0140e-02,  4.2559e-02, -3.4901e-02,  3.1951e-04, -8.0297e-02,\n",
      "         -3.2747e-02,  2.8527e-02, -5.1366e-02,  1.0939e-01,  8.1933e-02,\n",
      "         -9.8404e-02, -9.3410e-02, -1.5129e-02,  4.5125e-02,  4.9417e-02,\n",
      "         -2.5187e-02,  1.5708e-02, -1.2929e-01,  5.3189e-03,  4.0234e-03,\n",
      "         -2.3457e-02, -6.7298e-02,  2.9228e-02, -2.6085e-02,  1.3062e-02,\n",
      "         -3.1166e-02, -4.8271e-02, -5.5886e-02, -3.8751e-02,  1.2001e-01,\n",
      "         -1.0392e-02,  4.8971e-02,  5.5354e-02,  4.4936e-02, -4.0098e-03,\n",
      "         -1.0296e-01, -2.9297e-02, -5.8340e-02,  2.7047e-02, -2.2017e-02,\n",
      "         -7.2224e-02, -4.1387e-02, -1.9330e-02,  2.7333e-03,  2.7696e-04,\n",
      "         -9.6759e-02, -1.0057e-01, -1.4192e-02, -8.0789e-02,  4.5393e-02,\n",
      "          2.4504e-02,  5.9761e-02, -7.3818e-02,  1.1984e-02, -6.6340e-02,\n",
      "         -7.6905e-02,  3.8516e-02, -5.5936e-33,  2.8001e-02, -5.6078e-02,\n",
      "         -4.8660e-02,  2.1557e-02,  6.0198e-02, -4.8140e-02, -3.5025e-02,\n",
      "          1.9331e-02, -1.7515e-02, -3.8921e-02, -3.8106e-03, -1.7029e-02,\n",
      "          2.8210e-02,  1.2829e-02,  4.7160e-02,  6.2103e-02, -6.4359e-02,\n",
      "          1.2929e-01, -1.3123e-02,  5.2307e-02, -3.7368e-02,  2.8909e-02,\n",
      "         -1.6898e-02, -2.3733e-02, -3.3349e-02, -5.1676e-02,  1.5536e-02,\n",
      "          2.0880e-02, -1.2537e-02,  4.5958e-02,  3.7272e-02,  2.8057e-02,\n",
      "         -5.9000e-02, -1.1699e-02,  4.9218e-02,  4.7033e-02,  7.3549e-02,\n",
      "         -3.7053e-02,  3.9846e-03,  1.0641e-02, -1.6148e-04, -5.2717e-02,\n",
      "          2.7593e-02, -3.9292e-02,  8.4472e-02,  4.8686e-02, -4.8587e-03,\n",
      "          1.7995e-02, -4.2857e-02,  1.2338e-02,  6.3995e-03,  4.0482e-02,\n",
      "          1.4889e-02, -1.5394e-02,  7.6295e-02,  2.3704e-02,  4.4524e-02,\n",
      "          5.0820e-02, -2.3126e-03, -1.8874e-02, -1.2334e-02,  4.6600e-02,\n",
      "         -5.6344e-02,  6.2993e-02, -3.1554e-02,  3.2491e-02,  2.3467e-02,\n",
      "         -6.5544e-02,  2.0171e-02,  2.5708e-02, -1.2387e-02, -8.3649e-03,\n",
      "         -6.6438e-02,  9.4307e-02, -3.5709e-02, -3.4248e-02, -6.6636e-03,\n",
      "         -8.0152e-03, -3.0971e-02,  4.3301e-02, -8.2140e-03, -1.5080e-01,\n",
      "          3.0769e-02,  4.0072e-02, -3.7929e-02,  1.9322e-03,  4.0053e-02,\n",
      "         -8.7708e-02, -3.6849e-02,  8.5796e-03, -3.1925e-02, -1.2526e-02,\n",
      "          7.3554e-02,  1.3474e-03,  2.0592e-02,  2.7110e-33, -5.1858e-02,\n",
      "          5.7836e-02, -9.1899e-02,  3.9442e-02,  1.0558e-01, -1.9691e-02,\n",
      "          6.1840e-02, -7.6347e-02,  2.4088e-02,  9.4005e-02, -1.1654e-01,\n",
      "          3.7120e-02,  5.2243e-02, -3.9586e-03,  5.7221e-02,  5.3285e-03,\n",
      "          1.2402e-01,  1.3902e-02, -1.1025e-02,  3.5605e-02, -3.3075e-02,\n",
      "          8.1657e-02, -1.5200e-02,  6.0559e-02, -6.0140e-02,  3.2610e-02,\n",
      "         -3.4830e-02, -1.6988e-02, -9.7491e-02, -2.7148e-02,  1.7471e-03,\n",
      "         -7.6898e-02, -4.3186e-02, -1.8999e-02, -2.9166e-02,  5.7749e-02,\n",
      "          2.4182e-02, -1.1690e-02, -6.2144e-02,  2.8435e-02, -2.3751e-04,\n",
      "         -2.5178e-02,  4.3964e-03,  8.1284e-02,  3.6418e-02, -6.0401e-02,\n",
      "         -3.6552e-02, -7.9375e-02, -5.0853e-03,  6.6970e-02, -1.1778e-01,\n",
      "          3.2374e-02, -4.7125e-02, -1.3446e-02, -9.4844e-02,  8.2495e-03,\n",
      "         -1.0675e-02, -6.8188e-02,  1.1182e-03,  2.4802e-02, -6.3589e-02,\n",
      "          2.8449e-02, -2.6130e-02,  8.5811e-02,  1.1468e-01, -5.3535e-02,\n",
      "         -5.6359e-02,  4.2601e-02,  1.0945e-02,  2.0958e-02,  1.0013e-01,\n",
      "          3.2605e-02, -1.8421e-01, -3.9321e-02, -6.9146e-02, -6.3810e-02,\n",
      "         -6.5639e-02, -6.4125e-03, -4.7961e-02, -7.6813e-02,  2.9538e-02,\n",
      "         -2.2995e-02,  4.1704e-02, -2.5005e-02, -4.5451e-03, -4.1714e-02,\n",
      "         -1.3229e-02, -6.3836e-02, -2.4647e-03, -1.3734e-02,  1.6898e-02,\n",
      "         -6.3040e-02,  8.9888e-02,  4.1817e-02, -1.8569e-02, -1.8044e-08,\n",
      "         -1.6800e-02, -3.2158e-02,  6.3038e-02, -4.1309e-02,  4.4482e-02,\n",
      "          2.0247e-03,  6.2959e-02, -5.1737e-03, -1.0044e-02, -3.0564e-02,\n",
      "          3.5267e-02,  5.5858e-02, -4.6712e-02,  3.4510e-02,  3.2958e-02,\n",
      "          4.3011e-02,  2.9436e-02, -3.0316e-02, -1.7111e-02,  7.3748e-02,\n",
      "         -5.4791e-02,  2.7752e-02,  6.2017e-03,  1.5880e-02,  3.4298e-02,\n",
      "         -5.1575e-03,  2.3508e-02,  7.5314e-02,  1.9284e-02,  3.3620e-02,\n",
      "          5.0910e-02,  1.5250e-01,  1.6421e-02,  2.7053e-02,  3.7516e-02,\n",
      "          2.1855e-02,  5.6633e-02, -3.9575e-02,  7.1231e-02, -5.4138e-02,\n",
      "          1.0377e-03,  2.1185e-02, -3.5631e-02,  1.0902e-01,  2.7653e-03,\n",
      "          3.1400e-02,  1.3842e-03, -3.4574e-02, -4.5928e-02,  2.8808e-02,\n",
      "          7.1691e-03,  4.8468e-02,  2.6102e-02, -9.4407e-03,  2.8217e-02,\n",
      "          3.4872e-02,  3.6910e-02, -8.5895e-03, -3.5321e-02, -2.4786e-02,\n",
      "         -1.9192e-02,  3.8071e-02,  5.9965e-02, -4.2229e-02],\n",
      "        [ 8.6439e-02,  1.0276e-01,  5.3946e-03,  2.0444e-03, -9.9634e-03,\n",
      "          2.5386e-02,  4.9288e-02, -3.0627e-02,  6.8725e-02,  1.0137e-02,\n",
      "          7.7540e-02, -9.0081e-02,  6.1062e-03, -5.6990e-02,  1.4171e-02,\n",
      "          2.8049e-02, -8.6846e-02,  7.6440e-02, -1.0349e-01, -6.7744e-02,\n",
      "          6.9995e-02,  8.4425e-02, -7.2491e-03,  1.0477e-02,  1.3402e-02,\n",
      "          6.7758e-02, -9.4209e-02, -3.7169e-02,  5.2262e-02, -3.1085e-02,\n",
      "         -9.6341e-02,  1.5772e-02,  2.5787e-02,  7.8525e-02,  7.8995e-02,\n",
      "          1.9152e-02,  1.6436e-02,  3.1009e-03,  3.8131e-02,  2.3709e-02,\n",
      "          1.0539e-02, -4.4065e-02,  4.4174e-02, -2.5873e-02,  6.1538e-02,\n",
      "         -4.0543e-02, -8.6414e-02,  3.1972e-02, -8.9067e-04, -2.4444e-02,\n",
      "         -9.1972e-02,  2.3394e-02, -8.3029e-02,  4.4151e-02, -2.4969e-02,\n",
      "          6.2302e-02, -1.3035e-03,  7.5140e-02,  2.4638e-02, -6.4724e-02,\n",
      "         -1.1773e-01,  3.8339e-02, -9.1177e-02,  6.3545e-02,  7.6274e-02,\n",
      "         -8.8024e-02,  9.5456e-03, -4.6972e-02, -8.4174e-02,  3.8882e-02,\n",
      "         -1.1439e-01,  6.2886e-03, -3.4936e-02,  2.3975e-02, -3.3132e-02,\n",
      "         -1.5724e-02, -3.7896e-02, -8.8125e-03,  7.0612e-02,  3.2807e-02,\n",
      "          2.0367e-03, -1.1228e-01,  6.7972e-03,  1.2277e-02,  3.3530e-02,\n",
      "         -1.3620e-02, -2.2549e-02, -2.2523e-02, -2.0320e-02,  5.0430e-02,\n",
      "         -7.4865e-02, -8.2282e-02,  7.6596e-02,  4.9339e-02, -3.7555e-02,\n",
      "          1.4463e-02, -5.7246e-02, -1.7995e-02,  1.0970e-01,  1.1946e-01,\n",
      "          8.0922e-04,  6.1706e-02,  3.2632e-02, -1.3078e-01, -1.4864e-01,\n",
      "         -6.1623e-02,  4.3389e-02,  2.6713e-02,  1.3979e-02, -3.9400e-02,\n",
      "         -2.5271e-02,  3.8774e-03,  3.5866e-02, -6.1542e-02,  3.7666e-02,\n",
      "          2.6757e-02, -3.8266e-02, -3.5479e-02, -2.3923e-02,  8.6798e-02,\n",
      "         -1.8406e-02,  7.7104e-02,  1.3986e-03,  7.0038e-02, -4.7788e-02,\n",
      "         -7.8982e-02,  5.1081e-02, -2.9987e-33, -3.9165e-02, -2.5621e-03,\n",
      "          1.6521e-02,  9.4894e-03, -5.6622e-02,  6.5778e-02, -4.7700e-02,\n",
      "          1.1166e-02, -5.7356e-02, -9.1626e-03, -2.1752e-02, -5.5953e-02,\n",
      "         -1.1142e-02,  9.3279e-02,  1.6676e-02, -1.3672e-02,  4.3439e-02,\n",
      "          1.8724e-03,  7.2995e-03,  5.1633e-02,  4.8061e-02,  1.3534e-01,\n",
      "         -1.7174e-02, -1.2970e-02, -7.5011e-02,  2.6111e-02,  2.6980e-02,\n",
      "          7.8305e-04, -4.8727e-02,  1.1784e-02, -4.5958e-02, -4.8321e-02,\n",
      "         -1.9567e-02,  1.9389e-02,  1.9881e-02,  1.6743e-02,  9.8780e-02,\n",
      "         -2.7409e-02,  2.3481e-02,  3.7023e-03, -6.1451e-02, -1.2123e-03,\n",
      "         -9.5047e-03,  9.2515e-03,  2.3844e-02,  8.6123e-02,  2.2679e-02,\n",
      "          5.4514e-04,  3.4713e-02,  6.2547e-03, -6.9278e-03,  3.9240e-02,\n",
      "          1.1567e-02,  3.2628e-02,  6.2216e-02,  2.7611e-02,  1.8688e-02,\n",
      "          3.5581e-02,  4.1180e-02,  1.5478e-02,  4.2269e-02,  3.8225e-02,\n",
      "          1.0031e-02, -2.8325e-02,  4.4705e-02, -4.1046e-02, -4.5055e-03,\n",
      "         -5.4473e-02,  2.6232e-02,  1.7986e-02, -1.2312e-01, -4.6695e-02,\n",
      "         -1.3591e-02,  6.4671e-02,  3.5735e-03, -1.2223e-02, -1.7938e-02,\n",
      "         -2.5550e-02,  2.3722e-02,  4.0867e-03, -6.5148e-02,  4.4365e-02,\n",
      "          4.6860e-02, -3.2517e-02,  4.0227e-03, -3.9760e-03,  1.1194e-02,\n",
      "         -9.9560e-02,  3.3317e-02,  8.0106e-02,  9.4269e-02, -6.3829e-02,\n",
      "          3.2315e-02, -5.1355e-02, -7.4988e-03,  5.3005e-34, -4.1319e-02,\n",
      "          9.4965e-02, -1.0640e-01,  4.9659e-02, -3.4191e-02, -3.1675e-02,\n",
      "         -1.7156e-02,  1.7010e-03,  5.7976e-02, -1.2178e-03, -1.6854e-02,\n",
      "         -5.1691e-02,  5.5300e-02, -3.4265e-02,  3.0818e-02, -3.1048e-02,\n",
      "          9.2753e-02,  3.7266e-02, -2.3740e-02,  4.4589e-02,  1.4615e-02,\n",
      "          1.1624e-01, -5.0011e-02,  3.8872e-02,  4.2474e-03,  2.5698e-02,\n",
      "          3.2724e-02,  4.2991e-02, -1.3614e-02,  2.5612e-02,  1.0626e-02,\n",
      "         -8.4686e-02, -9.5298e-02,  1.0840e-01, -7.5160e-02, -1.3777e-02,\n",
      "          6.3734e-02, -4.4967e-03, -3.2532e-02,  6.2361e-02,  3.4805e-02,\n",
      "         -3.5492e-02, -2.0022e-02,  3.6661e-02, -2.4884e-02,  1.0182e-02,\n",
      "         -7.0123e-02, -4.3195e-02,  2.9533e-02, -2.9493e-04, -3.4539e-02,\n",
      "          1.4668e-02, -9.8397e-02, -4.7049e-02, -8.8549e-03, -8.8991e-02,\n",
      "          3.5100e-02, -1.2960e-01, -4.9887e-02, -6.1205e-02, -5.9780e-02,\n",
      "          9.4632e-03,  4.9122e-02, -7.7503e-02,  8.0973e-02, -4.7926e-02,\n",
      "          2.3438e-03,  7.5703e-02, -2.4018e-02, -1.5255e-02,  4.8674e-02,\n",
      "         -3.8597e-02, -7.0483e-02, -1.2035e-02, -3.8879e-02, -7.7602e-02,\n",
      "         -1.0724e-02,  1.0419e-02, -2.1375e-02, -9.1739e-02, -1.1134e-02,\n",
      "         -2.9607e-02,  2.4646e-02,  4.6571e-03, -1.6345e-02, -3.9522e-02,\n",
      "          7.7337e-02, -2.8473e-02, -3.6994e-03,  8.2767e-02, -1.1041e-02,\n",
      "          3.1398e-02,  5.3509e-02,  5.7515e-02, -3.1762e-02, -1.5291e-08,\n",
      "         -7.9966e-02, -4.7680e-02, -8.5979e-02,  5.6962e-02, -4.0887e-02,\n",
      "          2.2383e-02, -4.6445e-03, -3.8013e-02, -3.1067e-02, -1.0728e-02,\n",
      "          1.9770e-02,  7.7700e-03, -6.0947e-03, -3.8638e-02,  2.8027e-02,\n",
      "          6.7814e-02, -2.3535e-02,  3.2175e-02,  8.0254e-03, -2.3911e-02,\n",
      "         -1.2200e-03,  3.1460e-02, -5.2492e-02, -8.0681e-03,  3.1477e-03,\n",
      "          5.1150e-02, -4.4410e-02,  6.3601e-02,  3.8508e-02,  3.3043e-02,\n",
      "         -4.1872e-03,  4.9559e-02, -5.6961e-02, -6.4971e-03, -2.4979e-02,\n",
      "         -1.6087e-02,  6.6229e-02, -2.0631e-02,  1.0805e-01,  1.6855e-02,\n",
      "          1.4381e-02, -1.3213e-02, -1.2939e-01,  6.9522e-02, -5.5577e-02,\n",
      "         -6.7541e-02, -5.4582e-03, -6.1359e-03,  3.9084e-02, -6.2878e-02,\n",
      "          3.7406e-02, -1.1657e-02,  1.2915e-02, -5.5250e-02,  5.1608e-02,\n",
      "         -4.3084e-03,  5.8025e-02,  1.8694e-02,  2.2781e-02,  3.2167e-02,\n",
      "          5.3798e-02,  7.0285e-02,  7.4931e-02, -8.4178e-02]])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1749b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
