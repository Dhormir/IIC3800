{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3800 TÃ³picos en CC - NLP UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.index = dict()\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.index\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.index[item]\n",
    "\n",
    "    def add(self, word, docid):\n",
    "        if word in self.index:\n",
    "            if docid in self.index[word]:\n",
    "                self.index[word][docid] += 1\n",
    "            else:\n",
    "                self.index[word][docid] = 1\n",
    "        else:\n",
    "            d = dict()\n",
    "            d[docid] = 1\n",
    "            self.index[word] = d\n",
    "\n",
    "    #frequency of word in document, Tf\n",
    "    def get_document_frequency(self, word, docid):\n",
    "        if word in self.index:\n",
    "            if docid in self.index[word]:\n",
    "                return self.index[word][docid]\n",
    "            else:\n",
    "                raise LookupError('%s not in document %s' % (str(word), str(docid)))\n",
    "        else:\n",
    "            raise LookupError('%s not in index' % str(word))\n",
    "\n",
    "    #number of documents that contain word, ni\n",
    "    def get_index_frequency(self, word):\n",
    "        if word in self.index:\n",
    "            return len(self.index[word])\n",
    "        else:\n",
    "            raise LookupError('%s not in index' % word)\n",
    "\n",
    "\n",
    "class DocumentLengthTable:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.table = dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table)\n",
    "\n",
    "    def add(self, docid, length):\n",
    "        self.table[docid] = length\n",
    "\n",
    "    def get_length(self, docid): #dl\n",
    "        if docid in self.table:\n",
    "            return self.table[docid]\n",
    "        else:\n",
    "            raise LookupError('%s not found in table' % str(docid))\n",
    "\n",
    "    def get_average_length(self): #avgdl\n",
    "        sum = 0 \n",
    "        for length in self.table.values():\n",
    "            sum += length\n",
    "        return float(sum) / float(len(self.table))\n",
    "\n",
    "\n",
    "def build_data_structures(corpus):\n",
    "    idx = InvertedIndex()\n",
    "    dlt = DocumentLengthTable()\n",
    "    for docid in corpus:\n",
    "\n",
    "        #build inverted index, sequential scan\n",
    "        for word in corpus[docid]:\n",
    "            idx.add(str(word), str(docid))\n",
    "\n",
    "        #build document length table\n",
    "        length = len(corpus[str(docid)])\n",
    "        dlt.add(docid, length)\n",
    "    return idx, dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "# It's also possible to try with a stemmer or to mix a stemmer and a lemmatizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words and len(t) > 2]\n",
    "        words += tokens\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class CorpusParser:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.regex = re.compile('^#\\s*\\d+')\n",
    "        self.corpus = dict()\n",
    "\n",
    "    def parse(self):\n",
    "        with open(self.filename) as f:\n",
    "            s = ''.join(f.readlines())\n",
    "        blobs = s.split('#')[1:]\n",
    "        for doc in blobs:\n",
    "            text = doc.split()\n",
    "            docid = text.pop(0)\n",
    "            words = tokenize(doc)\n",
    "            self.corpus[docid] = words\n",
    "\n",
    "    def get_corpus(self):\n",
    "        return self.corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "k1 = 1.2\n",
    "k2 = 100\n",
    "b = 0.75\n",
    "R = 0.0\n",
    "\n",
    "\n",
    "def score_BM25(n, f, qf, r, N, dl, avdl):\n",
    "    K = compute_K(dl, avdl)\n",
    "    first = log( ( (r + 0.5) / (R - r + 0.5) ) / ( (n - r + 0.5) / (N - n - R + r + 0.5)) )\n",
    "    second = ((k1 + 1) * f) / (K + f)\n",
    "    third = ((k2+1) * qf) / (k2 + qf)\n",
    "    return first * second * third\n",
    "\n",
    "\n",
    "def compute_K(dl, avdl):\n",
    "    return k1 * ((1-b) + b * (float(dl)/float(avdl)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(index, dlt, query):\n",
    "        query_result = dict()\n",
    "        query_words = tokenize(query)\n",
    "        for term in query_words:\n",
    "            if term in index:\n",
    "                doc_dict = index[term] # retrieve index entry\n",
    "                for docid, freq in doc_dict.items(): #for each document and its word frequency\n",
    "                    score = score_BM25(n=len(doc_dict), f=freq, qf=1, r=0, N=len(dlt), dl=dlt.get_length(docid), avdl=dlt.get_average_length()) # calculate score\n",
    "                    if docid in query_result: #this document has already been scored once\n",
    "                        query_result[docid] += score\n",
    "                    else:\n",
    "                        query_result[docid] = score\n",
    "        return query_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CorpusParser(filename='corpus.txt')\n",
    "cp.parse()\n",
    "corpus = cp.get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, dlt = build_data_structures(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_query(index, dlt, 'technical reports of the acm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = dict(sorted(results.items(), key=lambda item:item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['propos',\n",
       " 'input',\n",
       " 'output',\n",
       " 'convent',\n",
       " 'algol',\n",
       " 'report',\n",
       " 'subcommitte',\n",
       " 'algol',\n",
       " 'acm',\n",
       " 'program',\n",
       " 'languag',\n",
       " 'committe',\n",
       " 'cacm',\n",
       " 'mai',\n",
       " 'march']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['1086']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
