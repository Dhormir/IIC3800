{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3800 Tópicos en CC - NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- numpy 1.20.3\n",
    "- nltk 3.7\n",
    "- gensim 4.1.2\n",
    "- keras 2.9.0\n",
    "- tensorflow 2.9.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!jupyter notebook --NotebookApp.iopub_data_rate_limit=1e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wiki_50 = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9288908839225769),\n",
       " ('throne', 0.882325291633606),\n",
       " ('elizabeth', 0.8789501786231995),\n",
       " ('princess', 0.8767548203468323),\n",
       " ('daughter', 0.8705160617828369),\n",
       " ('prince', 0.8702554702758789),\n",
       " ('kingdom', 0.8607221841812134),\n",
       " ('eldest', 0.8595449328422546),\n",
       " ('monarch', 0.8584721684455872),\n",
       " ('widow', 0.8549265265464783)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_50.most_similar_cosmul(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.8236179351806641),\n",
       " ('queen', 0.7839043140411377),\n",
       " ('ii', 0.7746230363845825),\n",
       " ('emperor', 0.7736247777938843),\n",
       " ('son', 0.766719400882721),\n",
       " ('uncle', 0.7627150416374207),\n",
       " ('kingdom', 0.7542160749435425),\n",
       " ('throne', 0.7539913654327393),\n",
       " ('brother', 0.7492411136627197),\n",
       " ('ruler', 0.7434253692626953)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_50.similar_by_word('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truck'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_50.doesnt_match(['king', 'george', 'stephen', 'truck'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, results = glove_wiki_50.evaluate_word_analogies('questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.463717540798522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectors_glove_50 = np.asarray(glove_wiki_50.vectors)\n",
    "labels_glove_50 = np.asarray(glove_wiki_50.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50451 ,  0.68607 , -0.59517 , -0.022801,  0.60046 , -0.13498 ,\n",
       "       -0.08813 ,  0.47377 , -0.61798 , -0.31012 , -0.076666,  1.493   ,\n",
       "       -0.034189, -0.98173 ,  0.68229 ,  0.81722 , -0.51874 , -0.31503 ,\n",
       "       -0.55809 ,  0.66421 ,  0.1961  , -0.13495 , -0.11476 , -0.30344 ,\n",
       "        0.41177 , -2.223   , -1.0756  , -1.0783  , -0.34354 ,  0.33505 ,\n",
       "        1.9927  , -0.04234 , -0.64319 ,  0.71125 ,  0.49159 ,  0.16754 ,\n",
       "        0.34344 , -0.25663 , -0.8523  ,  0.1661  ,  0.40102 ,  1.1685  ,\n",
       "       -1.0137  , -0.21585 , -0.15155 ,  0.78321 , -0.91241 , -1.6106  ,\n",
       "       -0.64426 , -0.51042 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_glove_50[int(np.where(labels_glove_50 == 'king')[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "X_train_text, Y_train = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
    "X_test_text, Y_test  = fetch_20newsgroups(subset=\"test\", remove=('headers', 'footers', 'quotes'), return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "classes = np.unique(Y_train)\n",
    "\n",
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "# It's also possible to try with a stemmer or to mix a stemmer and a lemmatizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words and len(t) > 2]\n",
    "        words += tokens\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = []\n",
    "test_docs = []\n",
    "\n",
    "for raw_text in X_train_text:\n",
    "    text = tokenize(raw_text)\n",
    "    train_docs.append(text)\n",
    "    \n",
    "for raw_text in X_test_text:\n",
    "    text = tokenize(raw_text)\n",
    "    test_docs.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 50), (7532, 50))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_tokens = 50 ## Hyperparameter, input length\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_docs+test_docs)\n",
    "\n",
    "## Vectorizing data to keep 50 words per sample.\n",
    "X_train_vect = pad_sequences(tokenizer.texts_to_sequences(train_docs), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)\n",
    "X_test_vect  = pad_sequences(tokenizer.texts_to_sequences(test_docs), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)\n",
    "\n",
    "\n",
    "X_train_vect.shape, X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95077"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver configuraciones de Tokenizer en: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver configuraciones de pad_sequences en: https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 50\n",
    "\n",
    "glove_50_embeddings = np.zeros((len(tokenizer.index_word)+1, embed_len))\n",
    "\n",
    "for idx, word in tokenizer.index_word.items():\n",
    "    if word in labels_glove_50:\n",
    "        glove_50_embeddings[idx] = vectors_glove_50[int(np.where(labels_glove_50 == word)[0][0])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 50)            4753900   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               320128    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 20)                1300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,083,584\n",
      "Trainable params: 329,684\n",
      "Non-trainable params: 4,753,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Flatten, Input\n",
    "\n",
    "model = Sequential([\n",
    "                    Embedding(input_dim=len(tokenizer.index_word)+1, output_dim=embed_len,\n",
    "                              input_length=max_tokens, trainable=False, weights=[glove_50_embeddings]),\n",
    "                    Flatten(),\n",
    "                    Dense(128, activation=\"relu\"),\n",
    "                    Dense(64, activation=\"relu\"),\n",
    "                    Dense(20, activation=\"softmax\")\n",
    "                ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 2.0976 - accuracy: 0.3261 - val_loss: 1.8041 - val_accuracy: 0.4036\n",
      "Epoch 2/8\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 1.2308 - accuracy: 0.6085 - val_loss: 1.8951 - val_accuracy: 0.4072\n",
      "Epoch 3/8\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 0.7813 - accuracy: 0.7633 - val_loss: 2.0682 - val_accuracy: 0.4047\n",
      "Epoch 4/8\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 0.4821 - accuracy: 0.8634 - val_loss: 2.4093 - val_accuracy: 0.4045\n",
      "Epoch 5/8\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 0.3046 - accuracy: 0.9230 - val_loss: 2.7316 - val_accuracy: 0.4000\n",
      "Epoch 6/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.2048 - accuracy: 0.9514 - val_loss: 3.0503 - val_accuracy: 0.3954\n",
      "Epoch 7/8\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 0.1570 - accuracy: 0.9640 - val_loss: 3.2816 - val_accuracy: 0.3906\n",
      "Epoch 8/8\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 0.1312 - accuracy: 0.9669 - val_loss: 3.4937 - val_accuracy: 0.3947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f352058e2e0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vect, Y_train, batch_size=32, epochs=8, validation_data=(X_test_vect, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 3ms/step\n",
      "Test Accuracy : 0.3947158789166224\n",
      "\n",
      "Classification Report : \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.22      0.28      0.25       319\n",
      "           comp.graphics       0.31      0.31      0.31       389\n",
      " comp.os.ms-windows.misc       0.25      0.25      0.25       394\n",
      "comp.sys.ibm.pc.hardware       0.33      0.31      0.32       392\n",
      "   comp.sys.mac.hardware       0.25      0.28      0.27       385\n",
      "          comp.windows.x       0.31      0.30      0.30       395\n",
      "            misc.forsale       0.50      0.46      0.48       390\n",
      "               rec.autos       0.30      0.56      0.39       396\n",
      "         rec.motorcycles       0.40      0.35      0.37       398\n",
      "      rec.sport.baseball       0.60      0.56      0.58       397\n",
      "        rec.sport.hockey       0.62      0.59      0.61       399\n",
      "               sci.crypt       0.39      0.37      0.38       396\n",
      "         sci.electronics       0.25      0.23      0.24       393\n",
      "                 sci.med       0.66      0.64      0.65       396\n",
      "               sci.space       0.56      0.45      0.50       394\n",
      "  soc.religion.christian       0.47      0.57      0.51       398\n",
      "      talk.politics.guns       0.36      0.37      0.37       364\n",
      "   talk.politics.mideast       0.63      0.57      0.60       376\n",
      "      talk.politics.misc       0.30      0.23      0.26       310\n",
      "      talk.religion.misc       0.10      0.05      0.07       251\n",
      "\n",
      "                accuracy                           0.39      7532\n",
      "               macro avg       0.39      0.39      0.38      7532\n",
      "            weighted avg       0.40      0.39      0.39      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "Y_preds = model.predict(X_test_vect).argmax(axis=-1)\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, Y_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 50, 50)            4753900   \n",
      "                                                                 \n",
      " tf.math.reduce_mean (TFOpLa  (None, 50)               0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               6528      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 20)                1300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,769,984\n",
      "Trainable params: 16,084\n",
      "Non-trainable params: 4,753,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(max_tokens, ))\n",
    "embeddings_layer = Embedding(input_dim=len(tokenizer.index_word)+1, output_dim=embed_len,\n",
    "                             input_length=max_tokens, trainable=False, weights=[glove_50_embeddings])\n",
    "dense1 = Dense(128, activation=\"relu\")\n",
    "dense2 = Dense(64, activation=\"relu\")\n",
    "dense3 = Dense(len(classes), activation=\"softmax\")\n",
    "\n",
    "x = embeddings_layer(inputs)\n",
    "x = tensorflow.reduce_mean(x, axis=1) ### Averaged embeddings of tokens of each example\n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "outputs = dense3(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.2584 - accuracy: 0.3085 - val_loss: 1.8671 - val_accuracy: 0.4049\n",
      "Epoch 2/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.7050 - accuracy: 0.4481 - val_loss: 1.7079 - val_accuracy: 0.4525\n",
      "Epoch 3/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.5979 - accuracy: 0.4836 - val_loss: 1.6468 - val_accuracy: 0.4800\n",
      "Epoch 4/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.5422 - accuracy: 0.5039 - val_loss: 1.6306 - val_accuracy: 0.4786\n",
      "Epoch 5/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.5026 - accuracy: 0.5153 - val_loss: 1.6140 - val_accuracy: 0.4895\n",
      "Epoch 6/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.4753 - accuracy: 0.5278 - val_loss: 1.5973 - val_accuracy: 0.5008\n",
      "Epoch 7/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.4519 - accuracy: 0.5291 - val_loss: 1.5753 - val_accuracy: 0.5000\n",
      "Epoch 8/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.4310 - accuracy: 0.5373 - val_loss: 1.5972 - val_accuracy: 0.4943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35206da1f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vect, Y_train, batch_size=32, epochs=8, validation_data=(X_test_vect, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 2ms/step\n",
      "Test Accuracy : 0.49429102496016997\n",
      "\n",
      "Classification Report : \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.25      0.38      0.30       319\n",
      "           comp.graphics       0.43      0.47      0.45       389\n",
      " comp.os.ms-windows.misc       0.40      0.17      0.24       394\n",
      "comp.sys.ibm.pc.hardware       0.50      0.22      0.31       392\n",
      "   comp.sys.mac.hardware       0.29      0.34      0.31       385\n",
      "          comp.windows.x       0.42      0.51      0.46       395\n",
      "            misc.forsale       0.57      0.57      0.57       390\n",
      "               rec.autos       0.65      0.51      0.57       396\n",
      "         rec.motorcycles       0.33      0.64      0.43       398\n",
      "      rec.sport.baseball       0.74      0.66      0.70       397\n",
      "        rec.sport.hockey       0.83      0.75      0.79       399\n",
      "               sci.crypt       0.57      0.44      0.50       396\n",
      "         sci.electronics       0.39      0.41      0.40       393\n",
      "                 sci.med       0.76      0.72      0.74       396\n",
      "               sci.space       0.51      0.67      0.58       394\n",
      "  soc.religion.christian       0.55      0.73      0.62       398\n",
      "      talk.politics.guns       0.47      0.42      0.45       364\n",
      "   talk.politics.mideast       0.66      0.67      0.66       376\n",
      "      talk.politics.misc       0.46      0.24      0.32       310\n",
      "      talk.religion.misc       0.23      0.11      0.15       251\n",
      "\n",
      "                accuracy                           0.49      7532\n",
      "               macro avg       0.50      0.48      0.48      7532\n",
      "            weighted avg       0.51      0.49      0.49      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_preds = model.predict(X_test_vect).argmax(axis=-1)\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, Y_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 50, 50)            4753900   \n",
      "                                                                 \n",
      " tf.math.reduce_sum (TFOpLam  (None, 50)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               6528      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 20)                1300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,769,984\n",
      "Trainable params: 16,084\n",
      "Non-trainable params: 4,753,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(max_tokens, ))\n",
    "embeddings_layer = Embedding(input_dim=len(tokenizer.index_word)+1, output_dim=embed_len,\n",
    "                             input_length=max_tokens, trainable=False, weights=[glove_50_embeddings])\n",
    "dense1 = Dense(128, activation=\"relu\")\n",
    "dense2 = Dense(64, activation=\"relu\")\n",
    "dense3 = Dense(len(classes), activation=\"softmax\")\n",
    "\n",
    "x = embeddings_layer(inputs)\n",
    "x = tensorflow.reduce_sum(x, axis=1) ### Summed embeddings of tokens of each example\n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "outputs = dense3(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.3890 - accuracy: 0.3250 - val_loss: 1.8830 - val_accuracy: 0.4180\n",
      "Epoch 2/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6816 - accuracy: 0.4667 - val_loss: 1.7595 - val_accuracy: 0.4575\n",
      "Epoch 3/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.5613 - accuracy: 0.5004 - val_loss: 1.6905 - val_accuracy: 0.4713\n",
      "Epoch 4/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.4894 - accuracy: 0.5187 - val_loss: 1.6673 - val_accuracy: 0.4785\n",
      "Epoch 5/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.4364 - accuracy: 0.5294 - val_loss: 1.6645 - val_accuracy: 0.4858\n",
      "Epoch 6/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.4008 - accuracy: 0.5447 - val_loss: 1.6591 - val_accuracy: 0.4831\n",
      "Epoch 7/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.3590 - accuracy: 0.5565 - val_loss: 1.6490 - val_accuracy: 0.4814\n",
      "Epoch 8/8\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.3267 - accuracy: 0.5637 - val_loss: 1.6746 - val_accuracy: 0.4894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f353236a760>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vect, Y_train, batch_size=32, epochs=8, validation_data=(X_test_vect, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 2ms/step\n",
      "Test Accuracy : 0.4893786510886883\n",
      "\n",
      "Classification Report : \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.28      0.33      0.30       319\n",
      "           comp.graphics       0.41      0.52      0.46       389\n",
      " comp.os.ms-windows.misc       0.39      0.21      0.28       394\n",
      "comp.sys.ibm.pc.hardware       0.54      0.23      0.32       392\n",
      "   comp.sys.mac.hardware       0.32      0.36      0.34       385\n",
      "          comp.windows.x       0.43      0.43      0.43       395\n",
      "            misc.forsale       0.54      0.66      0.59       390\n",
      "               rec.autos       0.37      0.66      0.47       396\n",
      "         rec.motorcycles       0.46      0.50      0.48       398\n",
      "      rec.sport.baseball       0.72      0.63      0.67       397\n",
      "        rec.sport.hockey       0.80      0.75      0.77       399\n",
      "               sci.crypt       0.47      0.51      0.49       396\n",
      "         sci.electronics       0.43      0.30      0.35       393\n",
      "                 sci.med       0.67      0.72      0.69       396\n",
      "               sci.space       0.58      0.60      0.59       394\n",
      "  soc.religion.christian       0.56      0.65      0.60       398\n",
      "      talk.politics.guns       0.42      0.48      0.45       364\n",
      "   talk.politics.mideast       0.73      0.61      0.66       376\n",
      "      talk.politics.misc       0.40      0.29      0.34       310\n",
      "      talk.religion.misc       0.22      0.15      0.18       251\n",
      "\n",
      "                accuracy                           0.49      7532\n",
      "               macro avg       0.49      0.48      0.47      7532\n",
      "            weighted avg       0.50      0.49      0.48      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_preds = model.predict(X_test_vect).argmax(axis=-1)\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, Y_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
