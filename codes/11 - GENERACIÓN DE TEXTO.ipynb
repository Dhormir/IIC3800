{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-3800 Tópicos en CC - NLP UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- spacy 3.5.1\n",
    "- keras 2.9.0\n",
    "- tensorflow 2.9.1\n",
    "- pandas, pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>Tokenized_Story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dan's parents were overweight. Dan was overwei...</td>\n",
       "      <td>[dan, 's, parents, were, overweight, ., dan, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carrie had just learned how to ride a bike. Sh...</td>\n",
       "      <td>[carrie, had, just, learned, how, to, ride, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morgan enjoyed long walks on the beach. She an...</td>\n",
       "      <td>[morgan, enjoyed, long, walks, on, the, beach,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane was working at a diner. Suddenly a custom...</td>\n",
       "      <td>[jane, was, working, at, a, diner, ., suddenly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was talking to my crush today. She continued...</td>\n",
       "      <td>[i, was, talking, to, my, crush, today, ., she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank had been drinking beer. He got a call fr...</td>\n",
       "      <td>[frank, had, been, drinking, beer, ., he, got,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave was in the Bahamas on vacation. He decide...</td>\n",
       "      <td>[dave, was, in, the, bahamas, on, vacation, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny enjoyed going to the beach. As she stepp...</td>\n",
       "      <td>[sunny, enjoyed, going, to, the, beach, ., as,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sally was happy when her widowed mom found a n...</td>\n",
       "      <td>[sally, was, happy, when, her, widowed, mom, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dan hit his golf ball and watched it go. The b...</td>\n",
       "      <td>[dan, hit, his, golf, ball, and, watched, it, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  \\\n",
       "0  Dan's parents were overweight. Dan was overwei...   \n",
       "1  Carrie had just learned how to ride a bike. Sh...   \n",
       "2  Morgan enjoyed long walks on the beach. She an...   \n",
       "3  Jane was working at a diner. Suddenly a custom...   \n",
       "4  I was talking to my crush today. She continued...   \n",
       "5  Frank had been drinking beer. He got a call fr...   \n",
       "6  Dave was in the Bahamas on vacation. He decide...   \n",
       "7  Sunny enjoyed going to the beach. As she stepp...   \n",
       "8  Sally was happy when her widowed mom found a n...   \n",
       "9  Dan hit his golf ball and watched it go. The b...   \n",
       "\n",
       "                                     Tokenized_Story  \n",
       "0  [dan, 's, parents, were, overweight, ., dan, w...  \n",
       "1  [carrie, had, just, learned, how, to, ride, a,...  \n",
       "2  [morgan, enjoyed, long, walks, on, the, beach,...  \n",
       "3  [jane, was, working, at, a, diner, ., suddenly...  \n",
       "4  [i, was, talking, to, my, crush, today, ., she...  \n",
       "5  [frank, had, been, drinking, beer, ., he, got,...  \n",
       "6  [dave, was, in, the, bahamas, on, vacation, .,...  \n",
       "7  [sunny, enjoyed, going, to, the, beach, ., as,...  \n",
       "8  [sally, was, happy, when, her, widowed, mom, f...  \n",
       "9  [dan, hit, his, golf, ball, and, watched, it, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas, spacy\n",
    "\n",
    "train_stories = pandas.read_csv('example_train_stories.csv', encoding='utf-8')\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "def text_to_tokens(text_seqs):\n",
    "    token_seqs = [[word.lower_ for word in nlp(text_seq)] for text_seq in text_seqs]\n",
    "    return token_seqs\n",
    "\n",
    "train_stories['Tokenized_Story'] = text_to_tokens(train_stories['Story'])\n",
    "    \n",
    "train_stories[['Story','Tokenized_Story']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY SAMPLE (1272 total items):\n",
      "{'dan': 2, \"'s\": 3, 'parents': 4, 'were': 5, 'overweight': 6, '.': 7, 'was': 8, 'as': 9, 'well': 10, 'the': 11, 'doctors': 12, 'told': 13, 'his': 14, 'it': 15, 'unhealthy': 16, 'understood': 17, 'and': 18, 'decided': 19, 'to': 20, 'make': 21}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def make_vocab(token_seqs, min_freq=1):\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    \n",
    "    vocab = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    \n",
    "    vocab = {token:idx + 2 for idx,token in enumerate(vocab)}\n",
    "    vocab[u'<UNK>'] = 1 \n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    print(\"VOCABULARY SAMPLE ({} total items):\".format(len(vocab)))\n",
    "    print(dict(list(vocab.items())[:20]))\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = make_vocab(token_seqs=train_stories['Tokenized_Story'], min_freq=1)\n",
    "\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOKUP SAMPLE:\n",
      "{2: 'dan', 3: \"'s\", 4: 'parents', 5: 'were', 6: 'overweight', 7: '.', 8: 'was', 9: 'as', 10: 'well', 11: 'the', 12: 'doctors', 13: 'told', 14: 'his', 15: 'it', 16: 'unhealthy', 17: 'understood', 18: 'and', 19: 'decided', 20: 'to', 21: 'make'}\n"
     ]
    }
   ],
   "source": [
    "def get_vocab_lookup(vocab):\n",
    "    vocab_lookup = {idx: vocab_item for vocab_item, idx in vocab.items()}\n",
    "    vocab_lookup[0] = \"\" \n",
    "    print(\"LOOKUP SAMPLE:\")\n",
    "    print(dict(list(vocab_lookup.items())[:20]))\n",
    "    return vocab_lookup\n",
    "\n",
    "vocab_lookup = get_vocab_lookup(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Story</th>\n",
       "      <th>Story_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[dan, 's, parents, were, overweight, ., dan, w...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 2, 8, 6, 9, 10, 7, 11, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[carrie, had, just, learned, how, to, ride, a,...</td>\n",
       "      <td>[29, 30, 31, 32, 33, 20, 34, 22, 35, 7, 36, 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[morgan, enjoyed, long, walks, on, the, beach,...</td>\n",
       "      <td>[57, 58, 59, 60, 27, 11, 61, 7, 36, 18, 41, 62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[jane, was, working, at, a, diner, ., suddenly...</td>\n",
       "      <td>[76, 8, 77, 78, 22, 79, 7, 80, 22, 81, 82, 83,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, was, talking, to, my, crush, today, ., she...</td>\n",
       "      <td>[98, 8, 99, 20, 100, 101, 102, 7, 36, 103, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[frank, had, been, drinking, beer, ., he, got,...</td>\n",
       "      <td>[123, 30, 124, 125, 126, 7, 74, 25, 22, 127, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[dave, was, in, the, bahamas, on, vacation, .,...</td>\n",
       "      <td>[146, 8, 147, 11, 148, 27, 149, 7, 74, 19, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[sunny, enjoyed, going, to, the, beach, ., as,...</td>\n",
       "      <td>[169, 58, 170, 20, 11, 61, 7, 9, 36, 171, 121,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[sally, was, happy, when, her, widowed, mom, f...</td>\n",
       "      <td>[182, 8, 183, 159, 41, 184, 185, 160, 22, 186,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[dan, hit, his, golf, ball, and, watched, it, ...</td>\n",
       "      <td>[2, 203, 14, 204, 205, 18, 206, 15, 63, 7, 11,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Tokenized_Story  \\\n",
       "0  [dan, 's, parents, were, overweight, ., dan, w...   \n",
       "1  [carrie, had, just, learned, how, to, ride, a,...   \n",
       "2  [morgan, enjoyed, long, walks, on, the, beach,...   \n",
       "3  [jane, was, working, at, a, diner, ., suddenly...   \n",
       "4  [i, was, talking, to, my, crush, today, ., she...   \n",
       "5  [frank, had, been, drinking, beer, ., he, got,...   \n",
       "6  [dave, was, in, the, bahamas, on, vacation, .,...   \n",
       "7  [sunny, enjoyed, going, to, the, beach, ., as,...   \n",
       "8  [sally, was, happy, when, her, widowed, mom, f...   \n",
       "9  [dan, hit, his, golf, ball, and, watched, it, ...   \n",
       "\n",
       "                                          Story_Idxs  \n",
       "0  [2, 3, 4, 5, 6, 7, 2, 8, 6, 9, 10, 7, 11, 12, ...  \n",
       "1  [29, 30, 31, 32, 33, 20, 34, 22, 35, 7, 36, 37...  \n",
       "2  [57, 58, 59, 60, 27, 11, 61, 7, 36, 18, 41, 62...  \n",
       "3  [76, 8, 77, 78, 22, 79, 7, 80, 22, 81, 82, 83,...  \n",
       "4  [98, 8, 99, 20, 100, 101, 102, 7, 36, 103, 20,...  \n",
       "5  [123, 30, 124, 125, 126, 7, 74, 25, 22, 127, 1...  \n",
       "6  [146, 8, 147, 11, 148, 27, 149, 7, 74, 19, 20,...  \n",
       "7  [169, 58, 170, 20, 11, 61, 7, 9, 36, 171, 121,...  \n",
       "8  [182, 8, 183, 159, 41, 184, 185, 160, 22, 186,...  \n",
       "9  [2, 203, 14, 204, 205, 18, 206, 15, 63, 7, 11,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_idxs(token_seqs, vocab):\n",
    "    idx_seqs = [[vocab[token] if token in vocab else vocab['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=train_stories['Tokenized_Story'],\n",
    "                                             vocab=vocab)\n",
    "                                   \n",
    "train_stories[['Tokenized_Story', 'Story_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   22   28    7]\n",
      " [   0    0    0 ...   41   56    7]\n",
      " [   0    0    0 ...   41   75    7]\n",
      " ...\n",
      " [   0    0    0 ...  107   41    7]\n",
      " [   0    0    0 ...   11 1266    7]\n",
      " [   0    0    0 ...  176  121    7]]\n",
      "SHAPE: (100, 71)\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_stories['Story_Idxs']]) \n",
    "\n",
    "train_padded_idxs = pad_sequences(train_stories['Story_Idxs'], maxlen=max_seq_len + 1) \n",
    "print(train_padded_idxs) \n",
    "\n",
    "print(\"SHAPE:\", train_padded_idxs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Word</th>\n",
       "      <th>Output Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'s</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parents</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>were</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>overweight</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dan</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>was</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>overweight</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>well</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the</td>\n",
       "      <td>doctors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doctors</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>told</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>his</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>parents</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>it</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>was</td>\n",
       "      <td>unhealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>unhealthy</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>.</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>his</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>parents</td>\n",
       "      <td>understood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>understood</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>and</td>\n",
       "      <td>decided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>decided</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>to</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>make</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>a</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>change</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>.</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>they</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>got</td>\n",
       "      <td>themselves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>themselves</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>and</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dan</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>on</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>a</td>\n",
       "      <td>diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>diet</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Input Word Output Word\n",
       "0            -         dan\n",
       "1          dan          's\n",
       "2           's     parents\n",
       "3      parents        were\n",
       "4         were  overweight\n",
       "5   overweight           .\n",
       "6            .         dan\n",
       "7          dan         was\n",
       "8          was  overweight\n",
       "9   overweight          as\n",
       "10          as        well\n",
       "11        well           .\n",
       "12           .         the\n",
       "13         the     doctors\n",
       "14     doctors        told\n",
       "15        told         his\n",
       "16         his     parents\n",
       "17     parents          it\n",
       "18          it         was\n",
       "19         was   unhealthy\n",
       "20   unhealthy           .\n",
       "21           .         his\n",
       "22         his     parents\n",
       "23     parents  understood\n",
       "24  understood         and\n",
       "25         and     decided\n",
       "26     decided          to\n",
       "27          to        make\n",
       "28        make           a\n",
       "29           a      change\n",
       "30      change           .\n",
       "31           .        they\n",
       "32        they         got\n",
       "33         got  themselves\n",
       "34  themselves         and\n",
       "35         and         dan\n",
       "36         dan          on\n",
       "37          on           a\n",
       "38           a        diet\n",
       "39        diet           ."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(list(zip([\"-\"] + train_stories['Tokenized_Story'].loc[0],\n",
    "                          train_stories['Tokenized_Story'].loc[0])),\n",
    "                 columns=['Input Word', 'Output Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Dense, Embedding, TimeDistributed, Input\n",
    "\n",
    "def create_model(seq_input_len, n_input_nodes, embedding_dim, \n",
    "                 stateful=False, batch_size=None):\n",
    "    \n",
    "    \n",
    "    input_layer = Input(batch_shape=(batch_size, seq_input_len), name='input_layer')\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes, \n",
    "                                output_dim=embedding_dim, \n",
    "                                mask_zero=True, name='embedding_layer')(input_layer) #mask_zero ignora el padding\n",
    "    \n",
    "    output_layer = TimeDistributed(Dense(n_input_nodes, activation=\"softmax\"), \n",
    "                                   name='output_layer')(embedding_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_idxs.shape[-1] - 1, #substract 1 from matrix length because of offset \n",
    "                     n_input_nodes = len(vocab) + 1, # Add 1 to account for 0 padding\n",
    "                     embedding_dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 70)]              0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, 70, 300)          381900    \n",
      "                                                                 \n",
      " output_layer (TimeDistribut  (None, 70, 1273)         383173    \n",
      " ed)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 765,073\n",
      "Trainable params: 765,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_padded_idxs[:,:-1], y=train_padded_idxs[:, 1:, None], epochs=50, batch_size=5, verbose=False)\n",
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_model = create_model(seq_input_len=1,\n",
    "                               n_input_nodes=len(vocab) + 1,\n",
    "                               embedding_dim = 300,\n",
    "                               stateful=True, \n",
    "                               batch_size = 1)\n",
    "\n",
    "predictor_model.load_weights('model_weights.h5')\n",
    "\n",
    "test_stories = pandas.read_csv('data/example_test_stories.csv', encoding='utf-8')\n",
    "test_stories['Tokenized_Story'] = text_to_tokens(test_stories['Story'])\n",
    "test_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=test_stories['Tokenized_Story'], vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ending(idx_seq):\n",
    "    \n",
    "    end_of_sent_tokens = [\".\", \"!\", \"?\"]\n",
    "    generated_ending = []\n",
    "    \n",
    "    \n",
    "    for word in idx_seq:\n",
    "        p_next_word = predictor_model.predict(np.array(word)[None,None], verbose=False)[0,0]\n",
    "        \n",
    "    while not generated_ending or vocab_lookup[next_word] not in end_of_sent_tokens:\n",
    "        #Randomly sample a word from the current probability distribution\n",
    "        next_word = np.random.choice(a=p_next_word.shape[-1], p=p_next_word)\n",
    "        # Append sampled word to generated ending\n",
    "        generated_ending.append(next_word)\n",
    "        # Get probabilities for next word by inputing sampled word\n",
    "        p_next_word = predictor_model.predict(np.array(next_word)[None,None], verbose=False)[0,0]\n",
    "    \n",
    "    model.reset_states() \n",
    "    \n",
    "    return generated_ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL STORY: lars went out skateboarding today . he skateboarded to the skate park . his friends taught him how to do a new trick . it is a difficult trick but he 's going to keep practicing .\n",
      "GIVEN ENDING: tomorrow he 'll teach his friends something new too .\n",
      "GENERATED ENDING: they called the membership allows him a ride . \n",
      "\n",
      "INITIAL STORY: abby is an avid scuba diver . abby 's dream has always been to scuba dive at the great barrier reef . one day abby got a letter from her father in the mail . as abby opened the letter she began crying with joy .\n",
      "GIVEN ENDING: abby 's dad was sending her to the great barrier reef to dive in may.\n",
      "GENERATED ENDING: jeff invited his job and had to make lunch . \n",
      "\n",
      "INITIAL STORY: maggie was 100 years old . she knew her time was coming to an end soon . she gathered all of her family around her bed side . she told them her final goodbyes .\n",
      "GIVEN ENDING: maggie passed away minutes later .\n",
      "GENERATED ENDING: hal 's car off the new apartment she was thinking about her weight . \n",
      "\n",
      "INITIAL STORY: july is the season for peaches . we got two buckets of peaches this year . we made jam from the peaches . we also made peach sauce for ice cream !\n",
      "GIVEN ENDING: it was hard work but all the peaches are gone now .\n",
      "GENERATED ENDING: the new gym . \n",
      "\n",
      "INITIAL STORY: kim was heading to the olympics . she was a well trained athlete . a month before the games she got hurt in practice . she tried her best to get better .\n",
      "GIVEN ENDING: unfortunately she had to pull out .\n",
      "GENERATED ENDING: they are sold to wash her mom was a diet . \n",
      "\n",
      "INITIAL STORY: i woke up way too early . i wanted nothing more than to go back to sleep . my back started hurting and i could n't get comfortable . i rolled into every position imaginable .\n",
      "GIVEN ENDING: i finally gave up and got out of bed .\n",
      "GENERATED ENDING: since frank lots nostalgic . \n",
      "\n",
      "INITIAL STORY: my stomach was burning for several days . and i had trouble eating any kind of food . so my dad bought me a probiotic drink made with fermented cabbages . it tasted terrible .\n",
      "GIVEN ENDING: but it quickly helped my stomach .\n",
      "GENERATED ENDING: the egg was over 10000 views . \n",
      "\n",
      "INITIAL STORY: anne was wearing her brand new dress . it cost her a fortune but she thought it was worth it . as she walked down the street a man with a coffee cup tripped by her . spilled coffee flew toward her .\n",
      "GIVEN ENDING: anne dodged the beverage just in time sparing her dress .\n",
      "GENERATED ENDING: saw these deer like the trash and water and prayed to make a boat to him he instead called for weeks and shouted . \n",
      "\n",
      "INITIAL STORY: karen had two cats . her roommate had none . often the apartment smelled like cat urine . karen and her roommate got into constant argument .\n",
      "GIVEN ENDING: eventually karen moved out .\n",
      "GENERATED ENDING: during drove home and he had missed them played job and began chanting positive phrases . \n",
      "\n",
      "INITIAL STORY: it started out as a dark and gloomy rainy day . i had to go to the store to grab some food for dinner . on the way i was passed by several ambulances . as i got closer to the store i saw the ambulances were stopped .\n",
      "GIVEN ENDING: it was a horrible crash scene and two people were dead .\n",
      "GENERATED ENDING: they believed him . \n",
      "\n",
      "INITIAL STORY: i was doing mturk tasks this morning . i went to fb for a few minutes . i saw a post about jaden smith 's suicide . i found out it was a hoax .\n",
      "GIVEN ENDING: i decided to limit my fb viewing .\n",
      "GENERATED ENDING: i decided it and now she is alright though . \n",
      "\n",
      "INITIAL STORY: carol was trying to clean up around the apartment . she tried to vacuum . unfortunately it broke halfway through . carol did n't know how to fix it .\n",
      "GIVEN ENDING: she had to stop vacuuming entirely .\n",
      "GENERATED ENDING: he did n't come to keep it was cold one day it from our condo since but they 's head ! \n",
      "\n",
      "INITIAL STORY: tammy was excited for christmas . she loved decorating . she started celebrating early by making a gingerbread house . it was fully detailed .\n",
      "GIVEN ENDING: all of tammy 's friends thought it looked great .\n",
      "GENERATED ENDING: james was walking she forgot his dad took class . \n",
      "\n",
      "INITIAL STORY: tom worked in a store stock room . he had to pick up heavy things often . one time his back gave out . tom was n't able to continue working for the day .\n",
      "GIVEN ENDING: he was written up and warned .\n",
      "GENERATED ENDING: the ground . \n",
      "\n",
      "INITIAL STORY: gina posted her lemonade stand in front of her house . she spent all day sitting there and waiting for customers . several hours have past and no one showed up . gina had to throw away her lemonade jugs since they 're used .\n",
      "GIVEN ENDING: she lost a lot of her profit .\n",
      "GENERATED ENDING: luckily her car . \n",
      "\n",
      "INITIAL STORY: krista wanted to make a nice meal for nate . she went to the store and got pasta tomatoes garlic and chicken . she went home and made her famous pasta sauce . nate came over and brought wine .\n",
      "GIVEN ENDING: krista and nate enjoyed a very nice dinner !\n",
      "GENERATED ENDING: on craigslist . \n",
      "\n",
      "INITIAL STORY: sam was in a marathon . he was close to first place . all of a sudden he fell . he did n't even make the top ten .\n",
      "GIVEN ENDING: sam was terribly disappointed .\n",
      "GENERATED ENDING: john puts cream parlor out of a long line outside . \n",
      "\n",
      "INITIAL STORY: abby lived in a small city in florida . abby heard stories of local deer wandering into citizens yards . however abby had yet to ever see one . one day abby noticed a animal eating from her garden .\n",
      "GIVEN ENDING: abby was shocked to see a deer was eating her tomatoes .\n",
      "GENERATED ENDING: he even got my recliner away by mistake . \n",
      "\n",
      "INITIAL STORY: george was driving home . he was stuck behind a slow car . he tried to pass but that lane was taken up too . he tried flashing his high - beams but it did nothing .\n",
      "GIVEN ENDING: he had to wait the entire way .\n",
      "GENERATED ENDING: amy listened to pick up . \n",
      "\n",
      "INITIAL STORY: when i was a sophomore in college the occupy movement arrived . in my town college students were protesting with the homeless . i went one night to the protest site and met a boy . we left the protest together spray painted bill boards and drank .\n",
      "GIVEN ENDING: i learned that the protesters had no idea what they were doing .\n",
      "GENERATED ENDING: chip then waited eagerly for some soup missed . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for _, test_story in test_stories[:20].iterrows():\n",
    "    \n",
    "    ending_story_idx = len(list(nlp(test_story['Story']).sents)[-1])\n",
    "    print(\"INITIAL STORY:\", \" \".join(test_story['Tokenized_Story'][:-ending_story_idx]))\n",
    "    print(\"GIVEN ENDING:\", \" \".join(test_story['Tokenized_Story'][-ending_story_idx:]))\n",
    "    \n",
    "    generated_ending = generate_ending(test_story['Story_Idxs'][:-ending_story_idx])\n",
    "    generated_ending = \" \".join([vocab_lookup[word] if word in vocab_lookup else \"\"\n",
    "                                 for word in generated_ending]) \n",
    "    print(\"GENERATED ENDING:\", generated_ending, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
